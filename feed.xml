<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://tkaminsky.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://tkaminsky.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-06T04:01:09+00:00</updated><id>https://tkaminsky.github.io/feed.xml</id><title type="html">Thomas Kaminsky</title><subtitle>Computer Science @ Harvard SEAS. </subtitle><entry><title type="html">Lyapunov, Control Lyapunov, and Control Barrier Functions,</title><link href="https://tkaminsky.github.io/blog/2024/clfs-and-cbfs/" rel="alternate" type="text/html" title="Lyapunov, Control Lyapunov, and Control Barrier Functions,"/><published>2024-11-04T10:16:00+00:00</published><updated>2024-11-04T10:16:00+00:00</updated><id>https://tkaminsky.github.io/blog/2024/clfs-and-cbfs</id><content type="html" xml:base="https://tkaminsky.github.io/blog/2024/clfs-and-cbfs/"><![CDATA[<p>Lately, I’ve been running into a lot of work that uses Lyapunov functions to prove statements about the convergence of certain distributed algorithms. I have a limited background in control, and so I initially struggled to wrap my head around these ideas. I also found that it wasn’t super easy to find all of the information that I wanted in a digestible format online. To hopefully begin to remedy this, here is my attempt to give a commonsense explanation of Lyapunov Functions, Control Lyapunov Functions, and Control Barrier Functions, at a level relevant to multi-robot coordination research.</p> <p>This blog post is definitely a work in progress. I’m writing down stuff as I learn it, and at times I include proofs or intuition which aren’t in the source papers, and as such may posess errors. That said, if you have any corrections or other ideas, please feel free to contact me at <code class="language-plaintext highlighter-rouge">tkaminsky@g.harvard.edu</code>.</p> <h3 id="table-of-contents">Table of Contents</h3> <ul> <li><strong><a href="#part-i--lyapunov-functions">Part I: Lyapunov Functions</a></strong></li> <li><strong><a href="#part-ii-control-lyapunov-functions">Part II: Control Lyapunov Functions</a></strong></li> <li><strong><a href="#part-iii--control-barrier-functions">Part III: Control Barrier Functions</a></strong></li> <li><strong><a href="#part-iv--controlling-with-cbfs-and-clfs">Part IV: Controlling with CBFs and CLFs</a></strong></li> <li><strong><a href="#references--further-reading">References and Further Reading</a></strong></li> </ul> <h1 id="part-i--lyapunov-functions">Part I : Lyapunov Functions</h1> <p>Throughout this blog post, we’re working in the world of dynamical systems—that is, we assume that we have access to some system dynamics of the form</p> \[\overset{.}{x}(t) = F(x(t)) \;\; \forall t \in [0,\tau).\] <p>In particular, we care about these dynamics when $x$ is in some neighborhood around $0$, which we define as a set <span class="inlinecode">$\mathcal{D} \in \mathbb{R}^n$</span> such that $0 \in \mathcal{D}$. Let’s engineer $F$ so that $0$ is an <em>equilibrium point</em>, so</p> \[F(0) = 0.\] <p>This means that if $x(t) = 0$, then it will stay there forever, since its time derivative $F(0)=0$. Notice that if some other point $x^{*}$ is an equilibrium point, we can always just construct a shifted dynamics $F’(x) = F(x - x^*)$ so that $0$ is the equilibrium instead, so we just assume for the sake of simplicity that we’ve already shifted the equlibrium to 0. Then our goal is to find out whether this system is <em>stable</em>—that is, whether trajectories $x(t)$ stay close to equilibrium, converge to equlibrium, or converge <em>quickly</em> to equilibrium.</p> <p><a href="https://en.wikipedia.org/wiki/Aleksandr_Lyapunov">Aleksandr Lyapunov</a> (1857-1918) was a Russian mathematician who did a lot of work answering these questions. He defined a a few important notions of stability, including the following three:</p> <ul> <li><strong>Lyapunov Stability</strong>: $x(t) = 0$ is Lyapunov stable if, for all $\varepsilon &gt; 0$, there exists $\delta &gt; 0$ such that $\norm{x(0)} &lt; \delta \implies \norm{x(t)} &lt; \varepsilon \;\; \forall t$. <ul> <li>Intuitively, a lyapunov-stable solution will stay less than $\varepsilon$-far away from the equlibrium point if we place it $\delta$-far away at initialization.</li> </ul> </li> <li><strong>Asymptotic Stability</strong>: $x(t) = 0$ is Asymptotically stable if it is Lyapunov stable and, for some $\delta &gt; 0$, if $\norm{x(0)} &lt; \delta$, then $\underset{t \to \infty}{\text{lim}} x(t) = 0$. <ul> <li>That is, in addition to staying ‘close’ to equilibrium, our trajectories actually reach equilibrium in the limit, assuming they start out reasonably close to it.</li> </ul> </li> <li><strong>Exponential Stability</strong>: $x(t) = 0$ is exponentially stable if there are some $\alpha, \beta, \delta &gt; 0$ such that, if $\norm{x(0)} &lt; \delta$, then $\norm{x(t)} \leq \alpha \norm{x(0)}e^{-\beta t}$ for all $t$. <ul> <li>This looks like asymptotic stability, under the additional constraint that it converges quickly (exponentially-fast) to equilibrium.</li> </ul> </li> </ul> <p>So Lyapunov stability just says that $x(t)$ won’t get ‘too far away’ from $0$, asymptotic stability says that $x(t)$ will eventually reach exactly $x(t) \to 0$, and exponential stability says that $x(t) \to 0$ at an exponential rate.</p> <p>One of the big ideas from Lyapunov theory is that, if we want to figure out whether an equilibrium point satisfies one of these stability conditions, we don’t need to directly look at trajectories ${x(t)}$. Consider for example a pendulum, with equilibrium corresponding to the pendulum being at rest. One way to show that this equilibrium point is (Lyapunov) stable would be to consider the <em>mechanical energy</em> of the system—the sum of the system’s kinetic and potential energies—and show that <em>it</em> is bounded. In particular, we know that the mechanical energy of a closed system must be constant. Thus, if we take the maximum height of our pendelum to be the point where all of the mechanical energy is converted to potential energy (we swing as high as possible, until we have 0 velocity), we know that our pendelum can never swing higher! Thus, we have bounded the possible displacement from equilibrium for our pendelum system.</p> <p>Lyapunov generalizes this idea of the mechanical energy of a system by defining so-called <strong>Lyapunov Functions</strong>. The definition is very general, but in practice this can almost always be thought of as an ‘energy’ at each point in a trajectory:</p> <hr/> <h4 id="definition-lyapunov-function"><strong>Definition (Lyapunov Function):</strong></h4> <p>Let $\overset{.}{x}(t) = F(x(t)) $ be a dynamical system with $x(t)=0$ a fixed point, as defined above. Then a continuously differentiable function $V : \mathcal{D} \to \mathbb{R}$ is called a <strong>Lyapunov Function</strong> for the system if the following hold:</p> <ol> <li>$V(0) = 0$.</li> <li>$V(x) &gt; 0 \;\; \forall x \in \mathcal{D} \setminus \{0\}$.</li> <li>The directional derivative $D_xV(x) \cdot \overset{.}{x}(t) = D_x V(x) \cdot F(x(t)) \leq 0$.</li> </ol> <hr/> <p>Notice how these properties coincide with our understanding of energy. The first two require that $V$ is only $0$ at equilibrium, and otherwise positive—this reflects our intuition that a trajectory ‘at rest’ in a stable equilibrium should have no kinetic energy and potential energy which could be defined as 0. Any displacement from equilibrium should either result in changing velocity and/or reaching a higher potential energy, so we’d expect non-equilibrium positions to have positive mechanical energy. The final condition thus says that $x(t)$ is always moving towards a lower-energy state (or, at least, not moving to a higher-energy state). If this is true, then we know that our position is never ‘gaining energy’, and so it can only get so far away from its current position.</p> <p>Using Lyapunov functions, we can characterize the stability of equilibrium point. I won’t prove this result (see Haddad et al.’s <a href="https://www.jstor.org/stable/j.ctvcm4hws">textbook</a>, p. 137-140, for details), but the following is an important theorem of Lyapunov:</p> <hr/> <h4 id="theorem-lyapunov-stability"><strong>Theorem (Lyapunov Stability).</strong></h4> <p>Let $\overset{\cdot}{x}(t) = F(x(t))$ be a dynamical system defined as above. Then the following hold:</p> <ol> <li> <p>If A Lyapunov function $V$ exists for fixed point $0$, then $x(t) = 0$ is <em>Lyapunov stable</em>.</p> </li> <li> <p>If furthermore \(D_x V(x) \cdot F(x(t)) &lt; 0,\) then the equilibrium is <em>asymptotically stable</em>.</p> </li> <li> <p>Finally, if there are also $\alpha, \beta, \varepsilon &gt;0$, $p \geq 1$ such that</p> <ul> <li>$\alpha \norm{x}^p \leq V(x) \leq \beta \norm{x}^p$, and</li> <li>$D_xV(x) \cdot F(x(t)) \leq -\varepsilon V(x)$,</li> </ul> <p>then the point is <em>exponentially stable</em>.</p> </li> </ol> <hr/> <p>Thus, we see that stability can be determined by considering properties of the Lyapunov function, rather than by looking at the trajectory directly! There’s more to say about this, but it seems as though basic Lyapunov theory is pretty well-covered online already. Especially helpful sources for me were Haddad et al.’s textbook <a href="https://www.jstor.org/stable/j.ctvcm4hws">Nonlinear Dynamical Systems and Control: A Lyapunov-Based Approach</a>, Slotine and Li’s textbook <a href="https://lewisgroup.uta.edu/ee5323/notes/Slotine%20and%20Li%20applied%20nonlinear%20control-%20bad%20quality.pdf">Applied Nonlinear Control</a>, and this <a href="https://byjus.com/maths/lyapunov-functions/">blog post</a>.</p> <p>However, there is one final point which is worth thinking about before we move on. In particular, asymptotic stability only requires that there exists <em>some</em> ball $B_\delta(0)$ in which every trajectory converges to 0. It isn’t obvious exactly which balls work for this, and this matters a lot in later sections, when we want to make sure that our system will converge to the equilibrium from a broad range of starting positions.</p> <p>To start to get at this, note that condition (3) of the Lyapunov Function definition implies that the derivative $\frac{d V(x(t))}{dt}$ is non-positive. In other words, $V(x(t))$ is a nonincreasing function. Then we can use the fact that $V(x(t)) \leq V(x(0))$ for all $t$ to make the following statement:</p> <hr/> <h4 id="lemma--invariance-of-sublevel-sets-of-lyapunov-functions"><strong>Lemma : Invariance of Sublevel Sets of Lyapunov Functions</strong></h4> <p>Consider any Lyapunov function $V$, and a corresponding <em>sublevel set</em> $S_\beta = \{ x \in \mathbb{R}^n | V(x) \leq \beta \}$ satisfying $S_\beta \subseteq \mathcal{D}$. Then $S_\beta$ is <em>forward-invariant</em>. That is, if $x(0) \in S_\beta$, then $x(t) \in S_\beta $ for all $t$.</p> <p>Moreover, if property $(2)$ of Lyapunov’s Stability Theorem holds, then $x(t) \to 0$ for all $x(0) \in S_\beta$ if either:</p> <ol> <li>$\mathcal{D}$ is bounded, or</li> <li>$V(x)$ is <em>radially unbounded</em>—that is, as $\norm{x}\to\infty$, $V(x) \to \infty$.</li> </ol> <hr/> <p><strong><em>Proof</em></strong>.</p> <p>Say that this statement were false. Then, since the trajectory $x(t)$ is continuous, there must exist some time $s$ such that $x(s)$ is not in $S_\beta$. Then, since by construction $V(x(0)) &lt; \beta$ and $V(x(s)) &gt; \beta$, we find</p> \[\frac{ V(x(s)) - V(x(0)) } { s - 0 } := m &gt; \frac{\beta - \beta}{s} = 0.\] <p>Thus, there is some positive secant slope $m$ connecting these two points. But since $V$ and $x$ are both continuously differentiable by construction, $V(x(t))$ is as well, and so we can apply the mean value theorem to find that there must be some time $s_0$ such that</p> \[\frac{d V(x(t))}{dt} = D_xV(x) \cdot F(x(t)) = m &gt; 0,\] <p>which contradicts our definition of a Lyapunov function. Thus, we find that each $S_\beta$ must be forward-invariant.</p> <p>Consider then the second condition (I’m still working on this part: Come back later!).</p> <h1 id="part-ii-control-lyapunov-functions">Part II: Control Lyapunov Functions</h1> <p>For this part, we are going to tweak our setting slightly. Say now that we can apply a <em>control input</em> $u(t)$ which lets us effect the state $x(t)$—for example, say that we had a motor which could supply some force in a direction of our choosing at each timestep. Unless otherwise mentioned, we let $u : \mathcal{D} \to \mathcal{U} \subseteq \mathbb{R}^m$ be a <em>feedback control</em> for our system: that is, $u$ is a function of $x$, $u(x)$ is chosen to apply a correction at point $x$ which pushes it towards equilibrium, and $\mathcal{U}$ is the space of all possible inputs.</p> <p>With this setup, we have a dynamical system which is a function of both the state and control input:</p> \[\overset{.}{x}(t) = F(x(t), u(t)), \;\; \forall t \in [0, \tau).\] <p>Our goal is now to figure out whether our control input will drive the system to equlibrium—or, more ambitiously, to somehow derive an optimal controller for this dynamical system.</p> <p>One naive approach is to just choose some feedback controller $u(x)$ and let $\overset{\sim}{x} := [x,u(x)]$ be a new state vector. Then</p> \[\overset{.}{\overset{\sim}{x}}(t) = \overset{\sim}{F}(\overset{\sim}{x}(t)) := F(x(t), u(x))\] <p>is a dynamical system which can be analyzed using the usual Lyapunov functions from <a href="#part-i--lyapunov-functions">Part I</a>. However, this methodology has some major problems. Most pressingly, if it’s <em>hard</em> to randomly pick a $u(x)$ that stabilizes the system, then we have no way to figure out ‘good’ choices of $u$ other than just by plugging in a ton of options. This motivates Control Lyapunov Functions—we want to create an object which can allow us to <em>locate</em> good controls for a given problem.</p> <p>We formally define a Control Lyapunov Function as follows:</p> <hr/> <h4 id="definition-control-lyapunov-function-clf"><strong>Definition (Control Lyapunov Function (CLF)):</strong></h4> <p>Let $F(x(t), u(t))$ be a controlled dynamical system defined as above. Then a continuously differentiable function $V: \mathcal{D} \to \mathbb{R}$ is called a control Lyapunov function for the system if the following hold:</p> <ol> <li>$V(0) = 0$.</li> <li>$V(x) &gt; 0 \;\; \forall x \in \mathcal{D} \setminus {0}$.</li> <li>$ \underset{u \in \mathcal{U}}{\inf} \; D_xV(x) \cdot F(x, u) &lt; 0$ for all $x \in \mathcal{D} \setminus {0}$.</li> </ol> <hr/> <p>The first two conditions are identical to a normal Lyapunov function. The third is a little stranger. It requres that, for every point $x$, there exists some control input $u$ that causes the directional derivative to be negative. We can think of this as implying the existence of a stabilizing feedback controller. In particular, taking</p> \[u^*(x) \in \{u \in \mathcal{U} | D_xV(x) \cdot F(x,u) &lt; 0\} \;\; \forall x \in \mathcal{D},\] <p>where each of these sets is guaranteed to be nonempty by condition (3), we find that the dynamical system</p> \[\overset{.}{x}(t) = F(x(t), u^*(x(t)))\] <p>is asymptotically stable, since $V$ is a Lyapunov function satisfying criterion (2) from Lyapunov’s Stability Theorem. Conversely, if condition (3) is not met, then there must be some $x$ such that NO control can stabilize it, and so there does not exist a stable controller. Thus, we can interpret the existence of a CLF as a necessary and sufficient condition for the dynamical system to be asymptotically stabilizable using a feedback controller. We summarize this in the following theorem:</p> <hr/> <h4 id="theorem-stability-of-controlled-systems"><strong>Theorem (Stability of Controlled Systems)</strong></h4> <p>Let $\overset{.}{x}(t) = F(x(t),u)$ be a controlled dynamical system defined as above. Then this system is asymptotically stabilizable by a feedback controller if and only if there exists a Control Lyapunov Function for the system.</p> <hr/> <p>Now, in some sense, we’ve solved our problem of characterizing optimal controllers. If we can develop an algorithm which can sample members of $\{u \in \mathcal{U} | D_xV(x) \cdot F(x,u) &lt; 0\}$, then we can use this scheme to create a controller. However, it is not always clear how to do this in practice. In the following section, we identify a strategy for calculating optimal controllers in the special case of a linear controller.</p> <h3 id="special-case--linear-control">Special Case : Linear Control</h3> <p>Assume that we are working with an <em>affine control system</em> governed my the dynamics</p> \[\overset{.}{x}(t) = F(x(t)) + G(x(t)) u(t) \;\; \forall t \in [0,\tau)\] <p>for some $F : \mathbb{R}^n \to \mathbb{R}^n$ and $G: \mathbb{R}^n \to \mathbb{R}^{n \times m}$, both smooth. Assume also for the time being that the control space \(\mathcal{U} = \mathbb{R}^m\). For a simple example of such a system, say that $G(x) = I$ for all $x$. Then at each time our control input corresponds to moving the agent in any direction.</p> <p>In this setting, we can make a stronger statement about the existence of optimal control:</p> <hr/> <h4 id="theorem-affine-control-lyapunov-function"><strong>Theorem (Affine Control Lyapunov Function)</strong></h4> <p>Under an affine control system $\overset{.}{x}(t) = F(x(t)) + G(x(t)) u(t)$ defined as above, a positive-definite, radially unbounded function $V:\mathbb{R}^n \to \mathbb{R}$ is a Control Lyapunov Function if and only if</p> \[D_xV(x) \cdot F(x(t)) &lt; 0 \;\; \forall x \in \mathcal{R},\] <p>where</p> \[\mathcal{R} := \{x \in \mathbb{R}^n | x \neq 0, D_xV(x) \cdot G(x(t)) = \mathbf{0}\}.\] <hr/> <p><em><strong>Proof.</strong></em></p> <p>Consider condition (3) of a CLF, that is:</p> <p>$ \underset{u \in \mathcal{U}}{\inf} \; D_xV(x) \cdot F(x, u) &lt; 0$ for all $x \in \mathcal{D} \setminus \{0\}.$</p> <p>By plugging in the affine control to this expression, we see that it’s equivalent to</p> \[\underset{u \in \mathcal{U}}{\inf} \; D_xV(x) \cdot [F(x(t)) + G(x(t)) u(t)]\] \[= \underset{u \in \mathcal{U}}{\inf} \; \left[ D_xV(x) \cdot F(x(t)) + D_xV(x) \cdot G(x(t)) u(t) \right].\] <p>If $D_xV(x) \cdot G(x(t)) \neq \mathbf{0}$, then there must be some non-zero entry at index $i$ with sign $\varepsilon$. Then taking $u_i \to -\varepsilon \cdot \infty$, we see that the entire expression can approach $-\infty &lt; 0$.</p> <p>Thus, the only conditions that we care about occur when $D_xV(x) \cdot G(x(t)) = \mathbf{0}$—that is, for points in $\mathcal{R}$. In these cases, we require that</p> \[\underset{u \in \mathcal{U}}{\inf} \; D_xV(x) \cdot F(x(t)) &lt; 0,\] <p>which is exactly our result.</p> <p>With this lemma, we can explicitly design a controller which globally (for any $x \in \mathbb{R}$ stabilizes our system). We define the following controller:</p> <hr/> <h4 id="theorem--linear-feedback-controller"><strong>Theorem : Linear Feedback Controller</strong></h4> <p>For the affine control system $\overset{.}{x}(t)=F(x(t))+G(x(t)) u(t)$ defined above, there exists a feedback controller $\phi : \mathbb{R}^n \to \mathbb{R}^m$ given by the following:</p> \[\phi(x) = \begin{cases} - \left( c_0 + \frac{\alpha(x) + \sqrt{\alpha^2(x) + \left[ \beta^T(x) \beta(x) \right]^2}}{\beta^T(x) \beta(x)} \right) \beta(x), &amp; \beta(x) \neq 0, \\ 0 &amp; \beta(x) = 0, \end{cases}\] <p>where $c_0 \geq 0$ is any scalar,</p> \[\alpha(x) := D_xV(x) \cdot F(x),\] <p>and</p> \[\beta(x) := \left[ D_xV(x) \cdot G(x) \right]^T.\] <hr/> <p><strong><em>Proof.</em></strong></p> <p>We just plug into</p> \[D_xV(x) \cdot [F(x) + G(x) \phi(x)] = \alpha(x) + \beta^T(x) \phi(x).\] <p>If $\beta(x) = 0$, then we find $D_xV(x) = D_xV(x) \cdot F(x) &lt; 0$ by the Affine Control Lyapunov Theorem.</p> <p>Otherwise, we substitute in $\phi$ to find</p> \[= \alpha(x) - \beta^T(x) \beta(x) \left( c_0 + \frac{\alpha(x) + \sqrt{\alpha^2(x) + \left[ \beta^T(x) \beta(x) \right]^2}}{\beta^T(x) \beta(x)} \right)\] \[= -\beta^T(x) \beta(x) c_0 - \sqrt{\alpha^2(x) + \left[ \beta^T(x) \beta(x) \right]^2} &lt; 0,\] <p>since both of these terms are negative by construction. Thus, we find that the derivative is negative, and so this is indeed a stabilizing controller.</p> <p>But what’s happening here intuitively? Consider for simplicity when $c_0 = 0$.</p> <p>Then let’s consider two examples. First, say that $G(x) = I$; that is, we just move in direction $u$. Then $\beta(x) = D_xV(x)$, so our control input has us move in the direction of steepest descent in $V$. This looks like a gradient descent! We have some ‘strength’ given by the big constant term, which we then apply to the direction which moves us towards equilibrium the fastest. Notice that our construction ensures that our derivative $\frac{d V(x(t))}{dt}$ is $-\sqrt{\alpha^2(x) + \norm{\beta}^4}$. So we see that this is the rate at which we will move down $V$.</p> <p>As another example, say that we have a more constrained $G$ given by $G(x) = [1,0]^T$. Then our control input is a one-dimensional displacement in the $x$ axis. Then $\beta(x) = \frac{d V(x)}{d x_1}$. Thus, our optimal control has us moving in the direction that decreases $V$ as much as possible, given that we can only control along the $x$ axis. Namely, if the partial along the $x$ axis is positive, we move in the negative direction, and vice-versa. Thus, we see that this looks like a constrained version of the previous example,</p> <p>Thus, we find that we can create optimal feedback controllers for affine systems.</p> <h3 id="note--local-stabilizability">Note : Local Stabilizability</h3> <p>One question that seems relevant is, what if we can’t assume that $\mathcal{U} = \mathbb{R}^m$? I’ll fill this in later (Section TBD), but I think that the main idea is that obviously global stability could fail, because we might get too far away from the equilibrium to reasonably control there. However, if we are operating in a bounded set $\mathcal{D}$, then under certain regularity assumptions (like that everything we care about is Lipschitz on $\mathcal{D}$), we can bound the size of the optimal controller given above. Thus, we can create a controller constrained to $\mathcal{U}$ which is optimal, and perhaps we can even say the ‘weakest’ controller which can still work!</p> <h1 id="part-iii--control-barrier-functions">Part III : Control Barrier Functions</h1> <p>Thus far, we have mostly been concerned with studying the <em>stability</em> of equilibria and controlled equilibria. However, there are often other concerns worth accounting for during planning.</p> <p>In this section, we consider <em>safety</em> concerns in dynamical systems. In particular, it is possible that some $x(t)$ are ‘bad’ states—for example, some orientations of a robotic arm could cause it to hit an object, or an autonomous vehicle being in certain regions of space could be dangerous if those regions are also occupied by humans. In general, we could imagine assigning a ‘badness’ score to every point in our state space, with negative badness scores being avoided. Then we could cast the problem of ensuring safety as enforcing that we remain in ‘good’ zones; that is, those with positive score.</p> <p>This idea enables us to define a <em>Barrier Function</em> (technically, a Zeroing Barrier Function, see <a href="https://ieeexplore.ieee.org/abstract/document/7782377?casa_token=Su26OE1ZV1EAAAAA:KMlvhbhQq0wb76ixit1QMQgIVqZEQj8Vf4fTlfewe5_auWi48Zb-nSSJ7mxopBuDgF0bLT3BHw">Ames et al.</a>):</p> <hr/> <h4 id="definition--barrier-function"><strong>Definition : Barrier Function</strong></h4> <p>A continuously differentiable function $h : \mathcal{D} \to \mathbb{R}$ is called a <em>Barrier Function</em> for $\mathcal{C} = \{ x\in \mathbb{R}^n | h(x) \geq 0 \}$ if $\mathcal{C} \subseteq \mathcal{D}$ and there exists some strictly increasing function $\alpha : \mathbb{R} \to \mathbb{R}$ with $\alpha(0) = 0$ such that</p> \[\frac{d h(x(t))}{d t} = D_xh(x) \cdot F(x) \geq -\alpha(h(x)).\] <hr/> <p>This definition implies that, at the boundary $\partial \mathcal{C} = \{x \in \mathbb{R}^n | h(x) = 0 \}$,</p> \[\frac{d h(x(t))}{d t} \geq -\alpha(0) = 0,\] <p>so $h(x)$ will not leave $\mathcal{C}$ if it is already inside it. Thus, we see that this definition enforces forward invariance as we desire. In fact, we can actually get an even stronger condition. Decause $\alpha$ is strictly increasing, if we are outside of our set $\mathcal{C}$, then we find that</p> \[\frac{d h(x(t))}{d t} \geq -\alpha(0) &gt; 0,\] <p>so we will be travelling towards $\mathcal{C}$ whenever we are outside of it. This can be formalized as a Lyapunov function as follows:</p> <hr/> <h4 id="theorem--bf-to-lyapunov-function"><strong>Theorem : BF to Lyapunov Function</strong></h4> <p>Say that $h(x)$ is a Barrier Function for $\mathcal{C}$ with domain $\mathcal{D}$. Then define $V_\mathcal{C}$ as follows:</p> \[V_\mathcal{C}(x) = \begin{cases} 0, &amp; \text{ if } x \in \mathcal{C},\\ -h(x) &amp; \text{ if } x \in \mathcal{D} \setminus \mathcal{C}. \end{cases}\] <p>$V_\mathcal{C}$ is a Lyapunov function for our problem. Moreover, $\mathcal{C}$ is an asymptotically stable fixed set.</p> <hr/> <p>Here we generalize a Lyapunov function to a set, rather than a particular point—this causes no real issues, but it is a worthwhile exercise to make sure that everything still makes sense when we do this replacement.</p> <p>It actually turns out that the converse of this theorem is also true if $\mathcal{C}$ is nonempty and compact. That is, we also have that $\mathcal{C}$ is a forward-invariant set if and only if you can create a barrier function for it!</p> <p>However, Barrier Functions don’t allow us to steer our dynamical system to safe regions. To do this, assume now that we are in an affine control system</p> \[\overset{.}{x}(t) = F(x(t)) + G(x(t)) u(t) \;\; \forall t \in [0,\tau)\] <p>analogously to defining CLFs from LFs, we define the notion of a <em>Control Barrier Function</em>:</p> <hr/> <h4 id="definition--control-barrier-function"><strong>Definition : Control Barrier Function</strong></h4> <p>A continuously differentiable function $h : \mathcal{D} \to \mathbb{R}$ is called a <em>Control Barrier Function</em> for $\mathcal{C} = \{ x\in \mathbb{R}^n | h(x) \geq 0 \}$ if $\mathcal{C} \subseteq \mathcal{D}$ and there exists some strictly increasing function $\alpha : \mathbb{R} \to \mathbb{R}$ with $\alpha(0) = 0$ such that</p> \[\underset{u \in \mathcal{U}}{\text{sup}} \frac{d h(x(t))}{d t} = \underset{u \in \mathcal{U}}{\text{sup}} D_xh(x) \cdot F(x) + D_x h(x) \cdot G(x) u \geq -\alpha(h(x)).\] <hr/> <p>This looks really similar to our definition of the CLF. Now, we see that there must exist some control input that will render each induce a barrier function when the corresponding control is substituted in. The analysis here is exactly the same as in the CLF case (indeed, we can define a CLF as in the previous section as the negation of our CBF), so we omit it here. The important thing is that we can design a control which allows us to ensure that we remain in a safe set.</p> <h1 id="part-iv--controlling-with-cbfs-and-clfs">Part IV : Controlling With CBFs and CLFs</h1> <p>Let’s tie this all together by defining concrete</p> <p>First, say that we are given a target feedback controller $\phi(x)$. We want to derive a controller $u$ which is as close to $\phi$ as possible, but with the added constraint that it must be safe with respect to some set $\mathcal{C}$. We can get this by solving the <a href="https://en.wikipedia.org/wiki/Quadratic_programming">Quadratic Program</a> (QP):</p> \[\begin{align*} u(x) &amp;= \underset{u \in \mathcal{U}}{\text{arg min}} \frac{1}{2} \norm{u - \phi(x)}^2 \\ &amp; \text{ s.t. } D_xh(x) \cdot F(x) + D_x h(x) \cdot u \geq -\alpha (h(x)). \end{align*}.\] <p>According to work outside the scope of this blog post (see <a href="https://coogan.ece.gatech.edu/papers/pdf/amesecc19.pdf">Ames et al., 2019.</a>), this problem has a closed-form solution when $\mathcal{U} = \mathbb{R}^n$.</p> <p>More generally, we can use this strategy to assign a controller that both stabilizes the system (as determined by some CLF $V$) AND offers safety guaranteed by $h$. In particular, the following QP works:</p> \[\begin{align*} u(x) &amp;= \underset{(u, \delta) \in \mathbb{R}^{m+1}}{\text{arg min}} \frac{1}{2} u^T H(x) u + p \delta^2 \\ &amp; \text{ s.t. } D_x V(x) \cdot F(x) + D_x V(x) \cdot u \leq -\gamma (V(x)) + \delta\\ &amp; D_xh(x) \cdot F(x) + D_x h(x) \cdot u \geq -\alpha (h(x)), \end{align*}\] <p>where $\gamma$ is another class $\mathcal{K}$ function, $H(x)$ is any positive definite matrix (so that term drives $\norm{u} \to 0$) and $\delta$ is a relaxation parameter which ensures that the program is solvable. Having the term $p \delta^2$ in the constraint drives this relaxation to $0$. The solution to this problem is guaranteed to be safe, and is likely to be stable (more so as we take $\delta \to 0$).</p> <h1 id="references--further-reading">References + Further Reading</h1> <p>Here are a few of the books and papers that I found especially helpful. Especially clear writing is marked in <strong>bold</strong>.</p> <h4 id="textbooks">Textbooks:</h4> <ul> <li> <p><a href="https://lewisgroup.uta.edu/ee5323/notes/Slotine%20and%20Li%20applied%20nonlinear%20control-%20bad%20quality.pdf">Slotine and Li, 1991. Applied Nonlinear Control</a></p> </li> <li> <p><strong><a href="https://www.jstor.org/stable/j.ctvcm4hws">Haddad and Chellaboina, 2008. Nonlinear Dynamical Systems and Control: A Lyapunov-Based Approach</a></strong></p> </li> </ul> <h4 id="papers-chronological">Papers (Chronological):</h4> <ul> <li> <p><a href="https://www.sciencedirect.com/science/article/pii/0362546X83900494">Artstein, 1982. Stabilization with Relaxed Controls</a></p> </li> <li> <p><a href="https://epubs.siam.org/doi/abs/10.1137/0321028?casa_token=UBCu6QQcDNQAAAAA:gnQ5sEIIyS-uXT8bzuob6UIffSQjAbhfVmpw8qx_cxOJ-RmMJT25gE1Hb5iwxCJqXlYsIl9J">Sontag, 1983. A Lyapunov-Like Characterization of Asymptotic Controllability</a></p> </li> <li> <p><a href="http://www.sontaglab.org/FTPDIR/art-sycon8903.pdf">Sontag, 1989. A ‘Universal’ Construction of Artstein’s Theorem on Nonlinear Stabilization</a></p> </li> <li> <p><strong><a href="https://ieeexplore.ieee.org/abstract/document/7782377?casa_token=Su26OE1ZV1EAAAAA:KMlvhbhQq0wb76ixit1QMQgIVqZEQj8Vf4fTlfewe5_auWi48Zb-nSSJ7mxopBuDgF0bLT3BHw">Ames et al., 2016. Control Barrier Function Based Quadratic Programs for Safety Critical Systems</a></strong></p> </li> <li> <p><a href="https://coogan.ece.gatech.edu/papers/pdf/amesecc19.pdf">Ames et al., 2019. Control Barrier Functions: Theory and Applications</a></p> </li> <li> <p><a href="https://ieeexplore.ieee.org/abstract/document/9197109?casa_token=HEt2jergrKEAAAAA:oStQJxHycKbc4iCHloCpSi62P8oWoPqILA4k_tcUEYNrMS76EK40c4lyLcJSjdBnA5LlL7E">Capelli et al., 2020. Connectivity Maintenance: Global and Optimized Approach Through Control Barrier Functions</a></p> </li> <li> <p><a href="https://ieeexplore.ieee.org/abstract/document/10354416?casa_token=6jyiwNV7sCEAAAAA:5BLuAUeRw1ZmuZaxHnD_YfWKkL0wZqbVP8pHQNU8xamrJAbb9cJMJGvkxzNIYrNsIZ59Or0">Cavorsi et al., 2023. Multirobot Adversarial Resilience Using Control Barrier Functions</a></p> </li> </ul> ]]></content><author><name></name></author><category term="sample-posts"/><category term="lyapunov"/><category term="control"/><category term="dynamical-systems"/><summary type="html"><![CDATA[A brief introduction to Lyapunov theory as I currently understand it.]]></summary></entry><entry><title type="html">A Brief List of MOOCs</title><link href="https://tkaminsky.github.io/blog/2024/mooc-guide/" rel="alternate" type="text/html" title="A Brief List of MOOCs"/><published>2024-06-11T16:40:16+00:00</published><updated>2024-06-11T16:40:16+00:00</updated><id>https://tkaminsky.github.io/blog/2024/mooc-guide</id><content type="html" xml:base="https://tkaminsky.github.io/blog/2024/mooc-guide/"><![CDATA[<p>A lot of people right now seem to be interested in self-studying course material, and I think that Massive Open Online Courses (MOOCs) are a great way to organize your learning. We’re living in a sort of golden age for free online education, but it can be pretty difficult to navigate all of your options. Here I lay out a sort of guide for the types of online courses that I’ve encountered, what I understand as the strengths and limitations of each type, and some good first courses to get started with online learning.</p> <h2 id="the-official-ones"><strong>The Official Ones</strong></h2> <p>Perhaps the gold standard for MOOCs are those that re-upload college-level courses in their (near) entirety. These have the benefit of being credentialed by some big-name institutions and being really well-organized, but a lot of them don’t take advantage of their online formats, and it can still be a struggle to engage with them because there is so much less explicit feedback than in an in-person class.</p> <p>A lot of them also have annoying, optional `certificate’ programs, which seem to me like a bit of a waste of money. I’m not sure that a lot of employers value a certified online course as much as, say, a course taken at a university or technical school, so I’d usually stick to auditing it for free instead. I describe a few of the major sites below:</p> <h4 id="mit-opencourseware"><a href="https://ocw.mit.edu/">MIT OpenCourseWare</a></h4> <p>OpenCourseWare (OCW) is probably my favorite website in general for taking online courses. They have a large list of offerings, most of which are really well-documented, including problem sets, exams, and sections.</p> <p>However, it definitely has a few issues. Though there are over 1,200 courses listed at the time of writing, only 250 or so have lecture videos, with the rest mostly being lecture notes. To save yourself some pain, just filter for courses with recordings to begin with. I’ve also found that, though some courses are really beautifully recorded, other videos are a little bad—I sometimes find it hard to read the board, especially in older courses.</p> <p>The videos also aren’t uniformly distributed across subjects. Unsurprisingly, most of the offerings are in Computer Science, followed by math and physics. So if you’re interested in other one of these neglected fields, especially in the social sciences or humantities, this might not be the platform for you.</p> <p>Here are some example courses that I think do a great job:</p> <ul> <li><a href="https://ocw.mit.edu/courses/6-006-introduction-to-algorithms-fall-2011/pages/syllabus/">6.006: Introduction to Algorithms</a></li> <li><a href="https://ocw.mit.edu/courses/6-849-geometric-folding-algorithms-linkages-origami-polyhedra-fall-2012/pages/syllabus/">6.849: Geometric Folding Algorithms: Linkages, Origami, Polyhedra</a></li> <li><a href="https://ocw.mit.edu/courses/cms-611j-creating-video-games-fall-2014/pages/syllabus/">CMS.611J: Creating Video Games</a></li> </ul> <h4 id="mit-open-learning-library"><a href="https://openlearning.mit.edu/">MIT Open Learning Library</a></h4> <p>OLL is a sort of super-OCW, in that a lot of these courses are, or used to be, listed on OCW. They tend to be more polished and better-utilize online tools, but this also can result in a less rigorous experience, especially where homework is concerned. Still, the autograders and professionally-recorded lectures make the platform a really great way to have an organized experience.</p> <p>It also has a strange relationship with EdX—like half of the courses are listed there as well, and so it isn’t always obvious how to</p> <p>Here are some courses that you might be interested in:</p> <ul> <li><a href="https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/course/">6.036: Introduction to Machine Learning</a></li> <li><a href="https://openlearninglibrary.mit.edu/courses/course-v1:OCW+18.06SC+2T2019/about">18.06SC: Linear Algebra</a></li> <li><a href="https://mitxonline.mit.edu/courses/course-v1:MITxT+8.02.1x/?utm_source=openlearning&amp;utm_medium=referral&amp;utm_campaign=mitx_catalog">8.02.1x: Electricity and Magnetism: Electrostatics</a></li> </ul> <h4 id="coursera"><a href="https://www.coursera.org/">Coursera</a></h4> <p>One of the first massive repositories for online courses, Coursera has perhaps the largest catalog of college-level courses for free online. Like OLL, it has a certificate program that you can buy if you really want to, but you can learn basically everything for free. It also has a wider variety of courses than OCW, and they are taught by a huge number of schools, rather than just MIT.</p> <p>The website was created by professors at Stanford, so it has especially high-quality courses from west-coast schools. However, they also allow courses to be uploaded by any school, or even by companies, I’ve noticed that they vary a ton in quality. Some of the courses uploaded by companies, in particular, seem more like lessons in pro-tech jargon than useful college-level courses, so use at your own risk. They’ve also leaned really hard into certifications and ‘tracks’ which are expensive and make bold claims about ‘skilling-up’, which I find slightly overhyped.</p> <p>Still, there are a lot of interesting courses on offer. Here are a few:</p> <ul> <li><a href="https://www.coursera.org/learn/crypto">Cryptography I</a> (Stanford)</li> <li><a href="https://www.coursera.org/learn/introduction-to-calculus">Introduction to Calculus</a> (University of Sydney)</li> <li><a href="https://www.coursera.org/learn/mythology">Greek and Roman Mythology</a> (UPenn)</li> <li><a href="https://www.coursera.org/learn/game-theory-1">Game Theory</a> (Stanford)</li> </ul> <h4 id="edx"><a href="https://www.edx.org/?utm_source=google&amp;utm_campaign=18736834479&amp;utm_medium=cpc&amp;utm_term=edx&amp;hsa_acc=7245054034&amp;hsa_cam=18736834479&amp;hsa_grp=140243978342&amp;hsa_ad=631521652739&amp;hsa_src=g&amp;hsa_tgt=kwd-89882436&amp;hsa_kw=edx&amp;hsa_mt=e&amp;hsa_net=adwords&amp;hsa_ver=3&amp;gad_source=1&amp;gclid=CjwKCAjw65-zBhBkEiwAjrqRMEOBsQ6zedEbtu9jPhubnOaQc7q21glYqTX5wQE2d33lsrHlRsTv5BoCFt4QAvD_BwE">edX</a></h4> <p>edX was Harvard and MIT’s answer to Coursera in 2012. It is very similar to Coursera and OLL in structure</p> <p>In terms of cons, it suffers at times from a lack of rigor, though some courses, like the incredible CS50X, manage to be surprisingly similar to their in-person variants. Also, because they allow courses to be uploaded by any school, or even by companies, I’ve noticed that courses vary a ton in quality. Some of the courses uploaded by companies, in particular, seem more like lessons in pro-tech jargon than useful college-level courses.</p> <p>Here are some good courses to try out:</p> <ul> <li><a href="https://www.edx.org/learn/computer-science/harvard-university-cs50-s-introduction-to-computer-science?index=product&amp;queryID=dae63c7c0a87115e4045c23228b90ab4&amp;position=4&amp;results_level=second-level-results&amp;term=&amp;objectID=course-da1b2400-322b-459b-97b0-0c557f05d017&amp;campaign=CS50%27s+Introduction+to+Computer+Science&amp;source=edX&amp;product_category=course&amp;placement_url=https%3A%2F%2Fwww.edx.org%2Fsearch">CS50: Introduction to Computer Science</a> (Harvard)</li> <li><a href="https://www.edx.org/learn/architecture-history/massachusetts-institute-of-technology-a-global-history-of-architecture?index=product&amp;objectID=course-89508395-9972-49c6-9ff4-1e276f9cc83b&amp;webview=false&amp;campaign=A+Global+History+of+Architecture&amp;source=edX&amp;product_category=course&amp;placement_url=https%3A%2F%2Fwww.edx.org%2Flearn%2Farchitecture-history">A Global History of Architecture</a> (MIT)</li> <li><a href="https://www.edx.org/learn/neuroscience/harvard-university-fundamentals-of-neuroscience-part-1-the-electrical-properties-of-the-neuron?index=product&amp;queryID=558d4a3a0a5acd03c64e59f617cb681b&amp;position=1&amp;results_level=second-level-results&amp;term=neuroscience&amp;objectID=course-f8042b37-6a21-4afa-b9e2-6f51dcd694db&amp;campaign=Fundamentals+of+Neuroscience%2C+Part+1%3A+The+Electrical+Properties+of+the+Neuron&amp;source=edX&amp;product_category=course&amp;placement_url=https%3A%2F%2Fwww.edx.org%2Fsearch">Fundamentals of Neuroscience</a> (Harvard)</li> <li><a href="https://www.edx.org/learn/happiness/university-of-california-berkeley-the-science-of-happiness?index=product&amp;queryID=cda429aabf2c8455f4c7092ec152d84f&amp;position=2&amp;results_level=second-level-results&amp;term=&amp;objectID=course-73484215-4007-48cd-ba90-c945cde6030d&amp;campaign=The+Science+of+Happiness&amp;source=edX&amp;product_category=course&amp;placement_url=https%3A%2F%2Fwww.edx.org%2Fsearch">The Science of Happiness</a> (Berkeley)</li> </ul> <h2 id="the-independent-websites"><strong>The Independent Websites</strong></h2> <p>There also exist a number of courses offered by parties unaffiliated with Universities. Lots of the big names in this category—<a href="https://www.udemy.com/?utm_source=adwords-brand&amp;utm_medium=udemyads&amp;utm_campaign=Brand-Udemy_la.EN_cc.US_dev&amp;campaigntype=Search&amp;portfolio=BrandDirect&amp;language=EN&amp;product=Course&amp;test=&amp;audience=Keyword&amp;topic=&amp;priority=NotSpecified&amp;utm_content=deal4584&amp;utm_term=_._ag_137319648178_._ad_634190764968_._kw_udemy_._de_c_._dm__._pl__._ti_kwd-296956216253_._li_9033311_._pd__._&amp;matchtype=b&amp;gad_source=1&amp;gclid=CjwKCAjw65-zBhBkEiwAjrqRMI42Ly4kM0rau4SLEmNmrk1iOQwwwTGIv7rKMpZ1Zi6aGeRSocLRiBoCGc0QAvD_BwE">Udemy</a>, <a href="https://www.masterclass.com/">MasterClass</a>, etc.—aren’t free, so I haven’t tried them.</p> <p>Still, there are a few I’d recommend:</p> <h4 id="khan-academy"><a href="https://www.khanacademy.org/">Khan Academy</a></h4> <p>By far the msot important online learning resource for High Schoolers, Sal Khan created a really incredible resource as far back as 2008. It’s grown into a large and exciting platform, though it focuses more on course material for younger students, rather than advanced courses. Still, if you want to study a course that you missed in high school, this is probably the place I’d start.</p> <h4 id="codecademy"><a href="https://www.codecademy.com/?g_network=g&amp;g_productchannel=&amp;g_adid=624951457609&amp;g_locinterest=&amp;g_keyword=codecademy&amp;g_acctid=243-039-7011&amp;g_adtype=&amp;g_keywordid=kwd-41065460761&amp;g_ifcreative=&amp;g_campaign=account&amp;g_locphysical=9033311&amp;g_adgroupid=70946090375&amp;g_productid=&amp;g_source={sourceid}&amp;g_merchantid=&amp;g_placement=&amp;g_partition=&amp;g_campaignid=1955172604&amp;g_ifproduct=&amp;utm_id=t_kwd-41065460761:ag_70946090375:cp_1955172604:n_g:d_c&amp;utm_source=google&amp;utm_medium=paid-search&amp;utm_term=codecademy&amp;utm_campaign=US_Brand_Exact&amp;utm_content=624951457609&amp;g_adtype=search&amp;g_acctid=243-039-7011&amp;gad_source=1&amp;gclid=CjwKCAjw65-zBhBkEiwAjrqRMHqf6b4kf_93WgGFz7Sax60_fPlpdcuaOx0mC_Dh-TVmP5-hdUZLeRoCxigQAvD_BwE">Codecademy</a></h4> <p>Codecademy is one of the classic coding education websites, and it’s still a great way for people with no coding experience to get their feet wet. The courses are really well-organized, and you can do everything online without having to worry about setup on your computer, which can be a pain in other computer science courses.</p> <p>Still, it’s not a perfect service. I’ve found that more and more courses are getting locked behind paywalls, and there aren’t great ways to get really deep technical understanding. I’d think of the courses as quick introductions to spark your interest and learn the basic syntax of whatever language you’re interested in, but you’ll eventually need to take a more involved course, or start slogging through documentation.</p> <h2 id="the-overachievers"><strong>The Overachievers</strong></h2> <p>Very rarely, a course independently uploads all of its materials, independent of a larger platform. In my experience, these tend to be really high in quality, so if you find them, you’ll probably have a pretty good time. Still, as far as I know, there isn’t a great way to search for these sorts of courses, so it may be hard to find one you’re looking for.</p> <p>They also tend to be exclusive to computer science and related fields, for obvious reasons. Finally, if you are interested in a certificate for any reason, I doubt that these programs can give you one. Still, I think they’re a great option.</p> <p>Here are some of my favorites:</p> <ul> <li><a href="https://manipulation.csail.mit.edu/Fall2023/">6.4210: Robotic Manipulation</a> (MIT)</li> <li><a href="https://cs3110.github.io/textbook/cover.html">CS 3110: Data Structures and Functional Programming</a> (Cornell)</li> <li><a href="https://spinningup.openai.com/en/latest/">Spinning Up in Deep RL</a> (OpenAI)</li> </ul> <h2 id="conclusion"><strong>Conclusion</strong></h2> <p>I hope this was helpful! Feel free to send me any other platforms that you find success with, or any thoughts about the list.</p> <p>—Thomas</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="mooc"/><category term="cs"/><category term="learning"/><category term="education"/><summary type="html"><![CDATA[Some of my favorite hosting sites for online courses, plus some general thoughts about online learning.]]></summary></entry></feed>