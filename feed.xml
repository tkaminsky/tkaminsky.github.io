<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://tkaminsky.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://tkaminsky.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-05-13T20:33:11+00:00</updated><id>https://tkaminsky.github.io/feed.xml</id><title type="html">Thomas Kaminsky</title><subtitle>Computer Science @ Harvard SEAS. </subtitle><entry><title type="html">Matrix Calculus; Or, Throw Away The Matrix Cookbook!</title><link href="https://tkaminsky.github.io/blog/2025/matrix-calculus/" rel="alternate" type="text/html" title="Matrix Calculus; Or, Throw Away The Matrix Cookbook!"/><published>2025-05-13T01:00:00+00:00</published><updated>2025-05-13T01:00:00+00:00</updated><id>https://tkaminsky.github.io/blog/2025/matrix-calculus</id><content type="html" xml:base="https://tkaminsky.github.io/blog/2025/matrix-calculus/"><![CDATA[<style>.center-box{background-color:#ccc;padding:20px;text-align:center;border-radius:8px;width:50%}</style> <p>Over winter break, I spent some time watching Prof. <a href="https://math.mit.edu/~edelman/">Alan Edelman</a> and Prof. <a href="https://math.mit.edu/~stevenj/">Steven G. Johnson</a>’s course on <a href="https://ocw.mit.edu/courses/18-s096-matrix-calculus-for-machine-learning-and-beyond-january-iap-2023/">Matrix Calculus</a>. It was an IAP course at MIT, which I think means that it was taught informally to a broad audience over a shorter time-span. I’ve been looking for a course like this for a while——calculus had been a pain point for me for a while, and I found that analysis and algebra didn’t do much to help me out. To give some illustrative examples, here are some questions that troubled me throughout my high school, college, and graduate studies:</p> <ol> <li>I studied stats, and we’d often encounter problems whose solution was the total derivative of a function from vectors to scalars. This is fine, but often applying the ‘chain rule’ would require calculating derivatives of matrix terms as an intermediate step, which I never understood. Basically every derivative would end up being some stack of partials so it wasn’t mechanically hard to find, but I could never remember whether it should be transposed, how the dimensions worked out, or why we were even allowed to do this.</li> <li>On that note, it has never even been clear to me what $df/dA$ even <strong>is</strong> when $A$ is a matrix! Is it just a huge dictionary of partial derivatives? Does it have a ‘shape’? Can we ‘multiply’ it with other things?</li> <li>I really hate working with indices! I always get them mixed up, and even if I don’t, excessive indexing makes calculations tedious and error-prone. In general, I’m a firm believer that mathematical ‘broccoli’—stuff that you have to do to get where you want to go, but which causes no joy in the doing—should be avoided whenever possible. Yet how many times have I had to unpack three or four sums to differentiate an expression that can just be written as some compact matrix product?</li> <li>To step back all the way to high school, what in the world is implicit differentiation? Why does the chain rule require you to ‘multiply by the derivative of the inside’? Why can you sometimes treat $df/dx$ as a fraction, and sometimes not?</li> <li>What is a Jacobian? What is a derivative? What is a gradient? What is a partial derivative? Why do all of these things have different names, and why do they have different shapes? Why do I constantly have to transpose the gradient for it to do anything interesting?</li> </ol> <p>I think that the intuition of matrix calculus helps answer all of these questions in a pretty satisfying way. It’s helped me reduce my reliance on <a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf">The Matrix Cookbook</a> and other tools, and it’s really tightened my understanding of both linear algebra and calculus. The course itself covers a fair amount of content that isn’t directly connected to this theme, and because its audience had a variety of mathematical backgrounds, it sometimes goes through a lot of concepts that more experienced students may be comfortable skipping. Thus, I’ve decided to write a brief explainer covering my major takeaways from the course, as well as a few interesting examples.</p> <p>Here’s a list of topics covered for reference:</p> <h3 id="table-of-contents">Table of Contents</h3> <ul> <li><strong><a href="#part-i--differentials-and-derivatives">Part I: Differentials and Derivatives</a></strong> <ul> <li><em><a href="#differences-of-functions">Differences of Functions</a></em></li> <li><em><a href="#differentials-and-differentiability">Differentials and Differentiability</a></em></li> <li><em><a href="#derivatives-of-linear-functions">Derivatives of Linear Functions</a></em></li> </ul> </li> <li><strong><a href="#part-ii--the-chain-and-product-rules">Part II: The Chain and Product Rules</a></strong> <ul> <li><em><a href="#application--elementwise-functions">Elementwise Functions</a></em></li> <li><em><a href="#application--matrix-inverse">Matrix Inverses</a></em></li> </ul> </li> <li><strong><a href="#part-iii--tricky-derivatives">Part III: Tricky Derivatives</a></strong> <ul> <li><em><a href="#the-determinant">Determinants</a></em></li> <li><em><a href="#matrix-powers">Matrix Powers</a></em></li> </ul> </li> <li><strong><a href="#part-iv--insights-about-the-world">Part IV : Insights About the World</a></strong> <ul> <li><em><a href="#gradients-via-inner-products">Gradients via Inner Products</a></em></li> <li><em><a href="#matrix-gradient-ascent">Matrix Gradient Ascent</a></em></li> <li><em><a href="#kronecker-products">Kronecker Products</a></em></li> <li><em><a href="#implicit-differentiation">Implicit Differentiation</a></em></li> </ul> </li> </ul> <h1 id="part-i--differentials-and-derivatives">Part I : Differentials and Derivatives</h1> <p>To start the process of developing matrix calculus, we need to be clear about what we mean by a ‘derivative’. To appreciate how hard this could be, recognize that, in different contexts, the derivative could be thought of as any of the following:</p> <ul> <li>The slope of the line tangent to $x$.</li> <li>The instantaneous rate at which $f(x)$ is increasing at $x$.</li> <li>The direction of steepest ascent of $f$ (for functions $f: \mathbb{R}^n \to \mathbb{R}$——the ‘gradient’)</li> <li>The matrix whose $(i,j)$-th entry is the change of the $i$-th entry of $f$ with respect to the $j$-th entry of $x$, in the sense of the first two definitions (for functions $f: \mathbb{R}^n \to \mathbb{R}$).</li> <li>etc. etc. etc.</li> </ul> <p>If you’ve taken a multivariable calculus class, you probably know some form of the following statement, which is in some sense the most satisfying summary of all these interpretations:</p> <p><strong>The derivative of a function at $x$ is its <em>best linear approximation</em> at $x$</strong>.</p> <p>More strongly, we think that the derivative provides a basically <em>perfect</em> linear approximation, so long as we’re really close to $x$. This intuition remains correct throughout the blog post, but we are going to add some additional structure to make it as explicit and organized of a notion as possible.</p> <h3 id="differences-of-functions">Differences of Functions</h3> <p>To start, let’s say that we have some $x$ that we care about, and we want to see how $f(x)$ changes when you perturb it——say, by some amount $\Delta x$. Then a natural quantity to consider would be the difference between the perturbed and the original input. Let’s call this $\Delta f$:</p> \[\Delta f \triangleq f(x + \Delta x) - f(x).\] <p>Let’s play this game with a concrete function as an example: $f(x) = 4x^2-3x+1$. We find</p> \[\Delta f = f(x + \Delta x) - f(x) = 4(x+ \Delta x)^2-3(x+\Delta x)+1 -4x^2+3x-1\] \[=4x^2 + 8 x \Delta x + 4\Delta x^2 - 3x - 3 \Delta x + 1 - 4x^2 + 3x - 1\] \[= 8x \Delta x - 3 \Delta x + 4\Delta x^2\] \[= (8x - 3) \Delta x + 4\Delta x^2.\] <p>A few things immediately jump out. First, assuming that we hold $x$ constant, notice that we’ve constructed a <em>function</em> from input space to output space. That is,</p> \[\Delta f : \mathbb{R} \to \mathbb{R} \;\; \text{ is given by } \;\; \Delta f (\Delta x) = (8x - 3) \Delta x + 4\Delta x^2.\] <p>So $\Delta f$ is just the name of a function (like $f$), and $\Delta x$ is the name of its input. This is true no matter what we plug in for $\Delta x$; it doesn’t need to be <em>infinitesimal</em>, or even particularly small! Let’s say for example that $x = 0$. If I plug in $\Delta x = 5$, I get</p> \[\Delta f(5) = -3 (5) + 4 \cdot 5^2 = 100 - 15 = 85.\] <p>If I compare this to $f(5) - f(0)$ explicitly, I also get that</p> \[f(5) - f(0) = (100 - 15 + 1) - 1 = 85.\] <p>So we see that $\Delta f$ gives us exactly the change in $f$ from $f(x)$ when we change the input from $x$ to $x + \Delta x$! Moreover, just as a sanity check, notice that $\Delta f + f(x)$ gives us</p> \[\Delta f + f(x) = (8x - 3) \Delta x + 4\Delta x^2 + 4x^2 - 3x + 1\] \[= 1 - 3 (x + \Delta x) + 4 (\Delta x^2 + 2 x \Delta x + x^2) = 1 - 3 (x+\Delta x) + 4 (x + \Delta x)^2.\] <p>So we see that $\Delta f + f(x)$ is just $f$, reparametrized so that $\Delta x$ is our input instead, and so we’re now centering our function at $\Delta x = 0$ instead of $x = 0$!</p> <p>We can do this for any function, and we’ll get a corresponding function that exactly captures the change in $f$. It might not be so easy to write in a simple way (e.g. $f(x) = \sin(x) + \cos(x)$), but there’s nothing stopping us from defining it in principle.</p> <h3 id="differentials-and-differentiability">Differentials and Differentiability</h3> <p>However, you might notice that, at least in the example above, if $\Delta x$ <em>is</em> small, then some of these terms don’t matter. In particular, $\Delta x^2$ goes to $0$ way faster than $\Delta x$ does (e.g., if $\Delta x = 10^{-5}$, then $\Delta x = 10^{-10}$—only $.001\%$ of $\Delta x$!). Then if we choose some really tiny $\Delta x$, which we’ll call $dx$, then the corresponding tiny $\Delta f$ (which we’ll likewise call $d f$), is approximately given by</p> \[df \approx (8x - 3) dx.\] <p>I tend to think of the terms $df$ and $dx$, which we’ll call <em>differentials</em>, intuitively rather than formally, as some small vectors with nearly negligable norm (and negligable squared norm). But you can also think about $dx$ as just another name for $\Delta x$ indicating that it’s small, and $df$ as the ‘leading term’ in $\Delta f$—that is, a quantity satisfying</p> \[\Delta f = df + o(||dx||),\] <p>where \(o(\norm{dx})\) is any function such that</p> \[\underset{||dx|| \to 0}{\lim} \frac{o(||dx||)}{||dx||} = 0.\] <p>Then intuitively, we’d like for this $df$ to look basically like $\Delta f$ when $dx$ is small. We can formalize this to get at the notion of <em>differentiability</em> explicitly:</p> <hr/> <h4 id="definition-differentiability"><strong>Definition (Differentiability).</strong></h4> <p>Let $f : V \to W$ be some function between two normed vector spaces, each with a corresponding metric \(\norm{\cdot}_V, \norm{\cdot}_W\). We say that $f$ is <em>differentiable</em> at $x \in V$ if there exists some linear function $D_{f \leftarrow x} : V \to W$ such that</p> \[\underset{\norm{\Delta x}_V \to \mathbf{0}}{\text{lim}} f(x+ \Delta x) - f(x) \triangleq df = D_{f \leftarrow x}[d x].\] <p>We call the linear operator $D_{f \leftarrow x}$ the <em>derivative</em> of $f$ with respect to $x$. You might also know it as $\frac{df}{dx}$, $J$, $\nabla_x f$, and so on. Note also that, when this linear transformation is a matrix, vector, or scalar, we might overload notation by also referring to the associated matrix/vector/scalar of transformation as the derivative.</p> <hr/> <ul> <li><em>Note : I like this notation $D_{f \leftarrow x}$, even though I don’t really see it used often. It emphasizes that $D$ is just some linear transformation, which takes $x$ points to $f$ points. The reversed arrow direction is intended to make the chain rule more obvious, as we’ll be composing linear transformations soon (see <a href="#the-chain-and-product-rules">the chain rule section</a>).</em></li> </ul> <p>Recall that, if $V = \mathbb{R}^n$ and $W= \mathbb{R}^m$ for some $n,m$, then it must be possible to write $D_{x \to f}$ as a matrix product with $\Delta x$. This is going to turn out to be the familiar <em>Jacobian</em> from multivariable calculus.</p> <p>Let’s get more familiar with this idea through a few simple examples.</p> <h5 id="example-1-vt-x">Example 1: $v^T x$.</h5> <p>Let $f: \mathbb{R}^n \to \mathbb{R}$ be given by $f(x) = v^T x$ for some constant vector $v$. Then</p> \[f(x + dx) - f(x) = v^T(x + dx) - v^T x = v^T dx.\] <p>So we can say that $D_{f \leftarrow x} = v^T$. Note that, oddly enough, we find that</p> \[df = f(dx).\] <p>We also see that, if we had computed $\nabla f$ explicitly, we would’ve found $\nabla f = v$. Then we also have $df = \nabla f ^T dx$.</p> <h5 id="example-2-xt-x">Example 2: $x^T x$.</h5> <p>Let’s look at a slightly more complicated function: $f(x) = x^T x$. We see that</p> \[df = f(x + dx) - f(x) = (x + dx)^T (x+dx) - x^T x = x^T x + 2 x^T dx + dx^T dx - x^T x\] \[= 2x^T dx + 0,\] <p>since we established that $dx^T dx = \norm{dx}^2$ is negligable. Notice also that $D_{f \leftarrow x} = \nabla f^T = 2x^T$ in this case, so we again get that $df = \nabla f^T dx$!</p> <h5 id="example-3-ax">Example 3: $Ax$</h5> <p>Let $f: \mathbb{R}^n \to \mathbb{R}^m$ be given by $f(x) = Ax$. Then</p> \[df = A(x + dx) - A(x) = A(dx).\] <p>Once again, you’ll notice that $df = f(dx)$, and now $D_{f \leftarrow x} = A$, which is the jacobian of $f$! We can actually extend this result to an arbitrary linear function with ease:</p> <h3 id="derivatives-of-linear-functions">Derivatives of Linear Functions</h3> <hr/> <h4 id="theorem-derivative-of-linear-functions"><strong>Theorem (Derivative of Linear Functions)</strong></h4> <p>For any linear function $T : V \to V$,</p> \[dT = T(dx).\] <hr/> <p><em>Proof.</em></p> <p>By linearity,</p> \[dT = T(x + dx) - T(x) = T(x) - T(x) + T(dx) = T(dx). \blacksquare\] <p>Though this may seem like a simple result, it actually comes in really handy for a ton of problems. For example, here are some linear functions that come up often in practice:</p> <ul> <li>$\text{diag}(x)$, the matrix whose $(i,i)$-th entry is $x_i$, and all other entries 0.</li> <li>$\text{tr}(A)$, the sum of the diagonal entries $\sum_{i=1}^n A_{ii}$.</li> <li>The transpose operator $X^T$, or more generally permutations of indices of matrices or vectors.</li> <li>The ‘index-into’ function $[A]_{ij}$, which picks out an entry from a matrix.</li> <li>Most generally, any matrix-to-matrix function whose elements are linear functions of the input matrix’s elements.</li> </ul> <p>Let’s show this last one explicitly, just because it’s easy and covers a ton of cases (including every one of the above).</p> <p>Say \([M(X)]_{ij} = T_{ij}(X)\). for some linear functions \(\{ T_{ij}\}\). Then</p> \[[M(cX + Y)]_{ij} = T_{ij}(cX + Y) = c T_{ij}(X) + T_{ij}(Y)\] \[= c [M(X)]_{ij} + [M(Y)]_{ij},\] <p>so $M(cX+Y) = cM(X) + M(Y).$</p> <p>Thus, we see that it’s linear, and so the above theorem applies. $\blacksquare$</p> <h1 id="part-ii--the-chain-and-product-rules">Part II : The Chain and Product Rules</h1> <p>All of this manual differentiation is pretty tedius, and it will become harder to do as we begin to tackle more complicated functions. As such, we should develop some general rules for performing differentiation, as in single-variable calculus. The easiest to show is the chain rule:</p> <hr/> <h4 id="theorem-chain-rule"><strong>Theorem (Chain Rule)</strong></h4> <p>If $g : U \to V$, $f: V \to W$, and $h : U \to W$ is given by $h(x) = f(g(x))$, then</p> \[dh = D_{f \leftarrow g}[D_{g \leftarrow x}[dx]].\] <p>In particular, if $U,V,$ and $W$ are all spaces of real vectors, then their derivatives $D_{x \leftarrow g}$ and $D_{g \leftarrow f}$ are matrices, and so</p> \[dh = D_{f \leftarrow g} D_{g \leftarrow x} dx.\] <hr/> <p><em>Proof.</em></p> <p>Consider first $g(x+dx)$. Note that</p> \[g(x+dx) = g(x+dx) - g(x) + g(x) = dg + g.\] <p>Then</p> \[dh = f(g(x + dx)) - f(g(x))\] \[= f(dg + g) - f(g) = D_{f \leftarrow g}[dg] = D_{f \leftarrow g}[D_{f \leftarrow x}[dx]]\] <p>by the definition of a differential. $\blacksquare$</p> <p>This has a really intuitive interpretation—the linear transformation that takes $dx$ to $dh$ is just the transformation that takes $dx$ to $dg$, followed by that taking $dg$ to $dh$ ($df$). This also helps us recognize that differentials behave really nicely: if you ever have some differential $df$ in an expression, you can always just substitute in any defined $D_{f \leftarrow g}[dg]$ to change variables.</p> <p>Next, let’s prove the product rule:</p> <hr/> <h4 id="theorem-product-rule"><strong>Theorem (Product Rule)</strong></h4> <p>Let $f,g : V \to W$, and $h :V \to W$ be given by $h = f(x) g(x)$. Then</p> \[dh = D_{f \leftarrow x}[dx] g(x) + f(x) D_{g \leftarrow x}[dx].\] <p>You’ll notice that including this dependence on $x$ is kind of ugly; and, by the chain rule, we know that we can transform any expression in terms of some $dg$ into an expression in terms of $dx$ by just plugging in the differential definition. Then for the rest of the post, let’s just repress the dependence on $x$, with the understanding that we can plug in $dx$’s whenever we want.</p> <p>This lets us rewrite the rule in a more simple form:</p> \[dh = df g + f dg.\] <hr/> <ul> <li>Proof. *</li> </ul> \[dh = f(x + dx)g(x + dx) - f(x) g(x)\] \[= (df + f) (dg + g) - f g = df dg + f dg + df g + fg - fg\] \[= f dg + df g + df dg.\] <p>Finally, note that, since both $df$ and $dg$ have negligable square norm, so does $df dg$, as it also has norm of square order. Then</p> \[= f dg + df g + 0. \blacksquare\] <h3 id="application--matrix-inverse">Application : Matrix Inverse</h3> <p>We now show that these properties allow us to derive some more interesting derivatives. In particular, let</p> \[F = A^{-1}.\] <p>Then $AF = I$, so by the chain rule (and the fact that $dI = 0$),</p> \[d(AF) = dA F + A dF = 0\] \[\iff A dF = - dA F \iff dF = - A^{-1} dA A^{-1}.\] <h3 id="application--elementwise-functions">Application : Elementwise Functions</h3> <p>Now, let’s draw our attention to a popular set of functions. Namely, say that $F: \mathbb{R}^n \to \mathbb{R}^n$ is given by</p> \[[F(x)]_i = f(x_i) \; \forall i \in [n],\] <p>for some scalar function $f$. Then let’s compute its derivative. We have that</p> \[[F]_i = f,\] <p>so</p> \[[dF]_i = d[F_i] = df = \frac{df}{dx}(x_i) d x_i.\] <p>Thus, letting $\frac{df}{dx} = \left[\frac{df}{dx}(x_1), \dots, \frac{df}{dx}(x_n) \right]^T$, we find</p> \[dF = \frac{df}{dx} \odot dx = \text{diag} \left(\frac{df}{dx} \right) dx,\] <p>where $\odot$ is the Hadamard product (elementwise multiplication).</p> <h1 id="part-iii--tricky-derivatives">Part III : Tricky Derivatives</h1> <p>Though so far things have gone off mostly without a hitch, there’s still a fair amount of art to matrix calculus. Just as in normal calculus, we need to construct a few tough ‘rules’ for differentiating common functions, and then the chain and product rules will help us cover lots of others built up out of these pieces. The determinant and matrix powers are the next such ‘atomic rules’ for us to parse.</p> <h3 id="the-determinant">The Determinant</h3> <p>For this, we’ll use the definition of the determinant given below:</p> \[\text{det}(A) = \sum_{\sigma \in S_n} \text{sign}(\sigma) a_{1 \sigma(1)}\dots a_{n \sigma(n)}.\] <p>Consider in particular terms of the form $det(dA + \lambda I)$. We’re broadly interested in figuring out what terms in this determinant have linear $O(dA)$ terms, since higher-order terms will go to zero in the limit. To get us there, let’s first look at which terms in this big sum have $\lambda^n$? We need all $n$ diagonal elements, so the only one that works is $\sigma = \mathbf{1}$, so we find</p> \[\prod_i (dA_{ii} - \lambda) = \lambda^n + \lambda^{n-1} \sum_i dA_{ii} + \lambda^{n-2} \sum_i \sum_j dA_{ii} dA_{jj} + \dots\] <p>For any other permutation, at least two terms are purely $a_{ij}$ (since it’s impossible for only 1 index to be misplaced), meaning that it’s impossible to choose another term with only $dA^1$ terms.</p> <p>Then consider</p> \[\text{det}(I + dA).\] <p>As we showed above, this looks like</p> \[1^n + 1^{n-1} \text{tr}(dA) + [\text{terms involving } o(dA^2)].\] <p>Then</p> \[\text{det}(A+dA) = \text{det} (AI + AA^{-1} dA) = \text{det}(A)\text{det}(I + A^{-1} dA)\] <p>by properties of determinants,</p> \[= \text{det}(A)(1 + \text{tr}(A^{-1}dA)).\] <p>then</p> \[d[\text{det}(A)] = \text{tr}(\text{det(A)} A^{-1} dA).\] <h3 id="matrix-powers">Matrix Powers</h3> <p>Now, say that $F_n(A) = A^n$. We’ll solve this by induction.</p> <p>The chain rule gives us</p> <p>\(dA^2 = dA A + A dA = \sum_{i=0}^1 A^i dA A^{1-i}\).</p> <p>Assume for some $n$ that</p> \[dA^n = \sum_{i=0}^{n-1} A^i dA A^{n-1-i}.\] <p>Then by the chain rule,</p> \[dA^{n+1} = dA A^n + A d(A^n) = dA A^n + \sum_{i=0}^{n-1} A^{i+1} dA A^{(n+1) - 1 -i}\] \[= \sum_{i=0}^{(n+1) - 1} A^{i} dA A^{(n+1) - 1 -i},\] <p>as desired. $\blacksquare$</p> <p>There are other tricky derivative rules left to discover, but hopefully these are enough to get you started!</p> <h1 id="part-iv--insights-about-the-world">Part IV : Insights About the World</h1> <p>I want to spend the rest of this post unpacking some really nice inuition that this new formulation of the derivative gives us. Some of these ideas are really deep, while some are simple——the only common thread between all of them is that I think they’re cool, and help me understand something about math.</p> <h3 id="gradients-via-inner-products">Gradients via Inner Products</h3> <p>Let’s think about functions of the form $f(x) : \mathbb{R}^n \to \mathbb{R}$; so-called <em>functionals</em>. To look at these functions, recall this foundational theorem from linear algebra:</p> <hr/> <h4 id="theorem-riesz-representation-theorem"><strong>Theorem (Riesz Representation Theorem)</strong></h4> <p>For any linear functional $F : V \to \mathbb{R}$ from real vector space $V$ equipped with inner product $\langle \cdot, \cdot \rangle$ to $\mathbb{R}$ there exists a unique $\phi \in V$ such that</p> \[F(x) = \langle \phi, x \rangle \: \forall x \in V.\] <hr/> <p>This is a general fact (which extends way beyond real vector spaces), but it has a really nice implication for us. It turns out, we can actually define the gradient relative to this representation:</p> <hr/> <h4 id="definition-gradient"><strong>Definition (Gradient)</strong></h4> <p>For a function $F : V \to \mathbb{R}$, the <em>gradient</em> at $x$ $\nabla F(x)$ is the unique vector in $V$ satisfying</p> \[dF = \langle \nabla F(x), dx \rangle.\] <hr/> <p>This makes clear a lot of our examples from the first section, where derivatives seemed to be transposed gradients when we looked at functions from vectors to scalars.</p> <p>But it doesn’t stop there! Consider the function $F = \text{det}(A)$, which maps from matrices to scalars. Recall the Frobenius inner product $\langle A,B \rangle = \text{tr}(A^TB)$ We found in a previous section that</p> \[dF = \text{tr}(\text{det(A)} A^{-1} dA).\] <p>This lets us immediately find that</p> \[\nabla_A F = \text{det(A)} A^{-T}.\] <p>Moreover, this formulation allows us to perform gradient ascent/descent just like in the vector-valued case.</p> <h3 id="matrix-gradient-ascent">Matrix Gradient Ascent</h3> <p>Say that we’re at some point $x \in V$, and we seek to move in the direction where some function $F : V \to \mathbb{R}$ is increasing the quickest. If $V$ was some column vector, it’s already clear that this direction is given by the gradient. We now show that this remains true for any inner product space. Assume that $\norm{dx} = \epsilon$ is held constant, so we can just choose a maximizing direction to increase. Well, by the definition above, we find</p> \[dF = \langle \nabla F, dx \rangle,\] <p>so we seek</p> \[\underset{x}{\text{argmax}} \left\langle \nabla F, \epsilon\frac{x}{||x||} \right\rangle.\] <p>Well Cauchy-Schwarz says that this is maximized when $x$ points in the same direction as $\nabla F$!</p> <p>In fact, we can also recover the first-order necessary optimality conditions for constrained optimization in vector spaces of matrices and tensors.</p> <h3 id="kronecker-products">Kronecker Products</h3> <p>Recall the vector space isomorphism theorem——any vector space over the real numbers is isomorphic to some vector space $\mathbb{R}^n$. Well we can use this to make more exotic-looking derivatives into a more pallatable form.</p> <p>For instance, the MIT course emphasizes the following identity for transforming matrices into vectors:</p> \[(A \otimes B) \text{vec}(C) = \text{vec}(BCA^T),\] <p>where $\otimes$ is the <a href="https://en.wikipedia.org/wiki/Kronecker_product">Kronecker Product</a>. We can use this to turn ugly-looking matrix derivatives into nicer vector derivatives. For example, we saw that</p> \[d(A^TA) = dA^T A + A^T dA.\] <p>This could be annoying to work with, since the $dA$ shows up in two places. But by applying the identity, we find</p> \[dA^T A = I dA^T A = (A^T \otimes I) \text{vec}(dA^T) = (A^T \otimes I) P \text{vec}(dA)\] <p>for suitable permutation matrix $P$. Doing the same for the second term and combining yields</p> \[\text{vec}(d(A^TA)) = (A^T \otimes I) (P + I) \text{vec}(dA),\] <p>which makes our derivative a matrix, as desired.</p> <h3 id="implicit-differentiation">Implicit Differentiation</h3> <p>I was always really confused about why we could take expressions like</p> \[x^2 + y^2 = 1\] <p>and calculate ‘derivatives’ like this:</p> \[2x dx + 2y dy = 1 \iff \frac{dy}{dx} = - \frac{x}{y},\] <p>so-called ‘implicit’ differentiation. I think I’ve seen a proof before, but nothing that stuck with me. This linear transformation framing makes it really obvious what’s going on. We ALWAYS have</p> \[dx^2 = 2x dx\] <p>and</p> \[dy^2 = 2y dy,\] <p>(and of course $d1 = 0$), so it’s always true that</p> \[2xdx + 2y dy = 0\] <p>solving for $dy$ yields</p> \[dy = -\frac{x}{y} dx,\] <p>and so $-x/y$ is our derivative by definition. No funny business!</p> <h3 id="normal-vectors-to-level-curves">Normal Vectors to Level Curves</h3> <p>Something that we often see in optimization is a situation where we have a function $F : \mathbb{R}^n \to \mathbb{R}$ and define a level curve by</p> \[F(x) = C\] <p>for some $C \in \mathbb{R}$. A fact about these level surface is that, at any point $x$, the vector $\nabla F(x)$ is normal to this surface at $x$. This is really useful for many geometric applications——for example, computing projections.</p> <p>I’ve definitely seen some convincing proofs of this, but it’s worth noting that our new machinery makes this fact immediate. Differentiating the constraint and using the Riesz representation theorem yields</p> \[dF = \langle \nabla F, dx \rangle = 0.\] <p>Since $dx$ is constrained to keep $x$ on the level surface, this tells us that, locally, the surface is orthogonal to $\nabla F$, as desired.</p> <h1 id="conclusion">Conclusion</h1> <p>Those are most of my thoughts for the time being. I’d be excited to hear if anyone has thoughts on how to apply these ideas to more interesting problems, or if anyone’s found some issues in the post that I can correct. I’m writing all of this down as I learn it, so I am far from an expert—nevertheless, hopefully this was still useful as an introduction to doing calculus on matrices and beyond!</p> <p>In any case, thanks for reading.</p> <p>—Thomas</p>]]></content><author><name></name></author><category term="math"/><category term="calculus"/><category term="linear-algebra"/><category term="matrix-calculus"/><summary type="html"><![CDATA[A brief, non-technical explainer on performing vectorized differentiation.]]></summary></entry><entry><title type="html">Lyapunov, Control Lyapunov, and Control Barrier Functions</title><link href="https://tkaminsky.github.io/blog/2024/clfs-and-cbfs/" rel="alternate" type="text/html" title="Lyapunov, Control Lyapunov, and Control Barrier Functions"/><published>2024-11-04T10:16:00+00:00</published><updated>2024-11-04T10:16:00+00:00</updated><id>https://tkaminsky.github.io/blog/2024/clfs-and-cbfs</id><content type="html" xml:base="https://tkaminsky.github.io/blog/2024/clfs-and-cbfs/"><![CDATA[<p>Lately, I’ve been running into a lot of work that uses Lyapunov functions to prove statements about the convergence of certain distributed algorithms. I have a limited background in control, and so I initially struggled to wrap my head around these ideas. I also found that it wasn’t super easy to find all of the information that I wanted in a digestible format online. To hopefully begin to remedy this, here is my attempt to give a commonsense explanation of Lyapunov Functions, Control Lyapunov Functions, and Control Barrier Functions, at a level relevant to multi-robot coordination research.</p> <p>This blog post is definitely a work in progress. I’m writing down stuff as I learn it, and at times I include proofs or intuition which aren’t in the source papers, and as such may posess errors. That said, if you have any corrections or other ideas, please feel free to contact me at <code class="language-plaintext highlighter-rouge">tkaminsky@g.harvard.edu</code>.</p> <h3 id="table-of-contents">Table of Contents</h3> <ul> <li><strong><a href="#part-i--lyapunov-functions">Part I: Lyapunov Functions</a></strong></li> <li><strong><a href="#part-ii-control-lyapunov-functions">Part II: Control Lyapunov Functions</a></strong></li> <li><strong><a href="#part-iii--control-barrier-functions">Part III: Control Barrier Functions</a></strong></li> <li><strong><a href="#part-iv--controlling-with-cbfs-and-clfs">Part IV: Controlling with CBFs and CLFs</a></strong></li> <li><strong><a href="#references--further-reading">References and Further Reading</a></strong></li> </ul> <h1 id="part-i--lyapunov-functions">Part I : Lyapunov Functions</h1> <p>Throughout this blog post, we’re working in the world of dynamical systems—that is, we assume that we have access to some system dynamics of the form</p> \[\overset{.}{x}(t) = F(x(t)) \;\; \forall t \in [0,\tau).\] <p>In particular, we care about these dynamics when $x$ is in some neighborhood around $0$, which we define as a set <span class="inlinecode">$\mathcal{D} \in \mathbb{R}^n$</span> such that $0 \in \mathcal{D}$. Let’s engineer $F$ so that $0$ is an <em>equilibrium point</em>, so</p> \[F(0) = 0.\] <p>This means that if $x(t) = 0$, then it will stay there forever, since its time derivative $F(0)=0$. Notice that if some other point $x^{*}$ is an equilibrium point, we can always just construct a shifted dynamics $F’(x) = F(x - x^*)$ so that $0$ is the equilibrium instead, so we just assume for the sake of simplicity that we’ve already shifted the equlibrium to 0. Then our goal is to find out whether this system is <em>stable</em>—that is, whether trajectories $x(t)$ stay close to equilibrium, converge to equlibrium, or converge <em>quickly</em> to equilibrium.</p> <p><a href="https://en.wikipedia.org/wiki/Aleksandr_Lyapunov">Aleksandr Lyapunov</a> (1857-1918) was a Russian mathematician who did a lot of work answering these questions. He defined a a few important notions of stability, including the following three:</p> <ul> <li> <p><strong>Lyapunov Stability</strong>: $x(t) = 0$ is Lyapunov stable if, for all $\varepsilon &gt; 0$, there exists $\delta &gt; 0$ such that $\norm{x(0)} &lt; \delta \implies \norm{x(t)} &lt; \varepsilon \;\; \forall t$.</p> <ul> <li>Intuitively, a lyapunov-stable solution will stay less than $\varepsilon$-far away from the equlibrium point if we place it $\delta$-far away at initialization.</li> </ul> </li> <li> <p><strong>Asymptotic Stability</strong>: $x(t) = 0$ is Asymptotically stable if it is Lyapunov stable and, for some $\delta &gt; 0$, if $\norm{x(0)} &lt; \delta$, then $\underset{t \to \infty}{\text{lim}} x(t) = 0$.</p> <ul> <li>That is, in addition to staying ‘close’ to equilibrium, our trajectories actually reach equilibrium in the limit, assuming they start out reasonably close to it.</li> </ul> </li> <li> <p><strong>Exponential Stability</strong>: $x(t) = 0$ is exponentially stable if there are some $\alpha, \beta, \delta &gt; 0$ such that, if $\norm{x(0)} &lt; \delta$, then $\norm{x(t)} \leq \alpha \norm{x(0)}e^{-\beta t}$ for all $t$.</p> <ul> <li>This looks like asymptotic stability, under the additional constraint that it converges quickly (exponentially-fast) to equilibrium.</li> </ul> </li> </ul> <p>So Lyapunov stability just says that $x(t)$ won’t get ‘too far away’ from $0$, asymptotic stability says that $x(t)$ will eventually reach exactly $x(t) \to 0$, and exponential stability says that $x(t) \to 0$ at an exponential rate.</p> <p>One of the big ideas from Lyapunov theory is that, if we want to figure out whether an equilibrium point satisfies one of these stability conditions, we don’t need to directly look at trajectories ${x(t)}$. Consider for example a pendulum, with equilibrium corresponding to the pendulum being at rest. One way to show that this equilibrium point is (Lyapunov) stable would be to consider the <em>mechanical energy</em> of the system—the sum of the system’s kinetic and potential energies—and show that <em>it</em> is bounded. In particular, we know that the mechanical energy of a closed system must be constant. Thus, if we take the maximum height of our pendelum to be the point where all of the mechanical energy is converted to potential energy (we swing as high as possible, until we have 0 velocity), we know that our pendelum can never swing higher! Thus, we have bounded the possible displacement from equilibrium for our pendelum system.</p> <p>Lyapunov generalizes this idea of the mechanical energy of a system by defining so-called <strong>Lyapunov Functions</strong>. The definition is very general, but in practice this can almost always be thought of as an ‘energy’ at each point in a trajectory:</p> <hr/> <h4 id="definition-lyapunov-function"><strong>Definition (Lyapunov Function):</strong></h4> <p>Let $\overset{.}{x}(t) = F(x(t)) $ be a dynamical system with $x(t)=0$ a fixed point, as defined above. Then a continuously differentiable function $V : \mathcal{D} \to \mathbb{R}$ is called a <strong>Lyapunov Function</strong> for the system if the following hold:</p> <ol> <li>$V(0) = 0$.</li> <li>$V(x) &gt; 0 \;\; \forall x \in \mathcal{D} \setminus \{0\}$.</li> <li>The directional derivative $D_xV(x) \cdot \overset{.}{x}(t) = D_x V(x) \cdot F(x(t)) \leq 0$.</li> </ol> <hr/> <p>Notice how these properties coincide with our understanding of energy. The first two require that $V$ is only $0$ at equilibrium, and otherwise positive—this reflects our intuition that a trajectory ‘at rest’ in a stable equilibrium should have no kinetic energy and potential energy which could be defined as 0. Any displacement from equilibrium should either result in changing velocity and/or reaching a higher potential energy, so we’d expect non-equilibrium positions to have positive mechanical energy. The final condition thus says that $x(t)$ is always moving towards a lower-energy state (or, at least, not moving to a higher-energy state). If this is true, then we know that our position is never ‘gaining energy’, and so it can only get so far away from its current position.</p> <p>Using Lyapunov functions, we can characterize the stability of equilibrium point. I won’t prove this result (see Haddad et al.’s <a href="https://www.jstor.org/stable/j.ctvcm4hws">textbook</a>, p. 137-140, for details), but the following is an important theorem of Lyapunov:</p> <hr/> <h4 id="theorem-lyapunov-stability"><strong>Theorem (Lyapunov Stability).</strong></h4> <p>Let $\overset{\cdot}{x}(t) = F(x(t))$ be a dynamical system defined as above. Then the following hold:</p> <ol> <li> <p>If A Lyapunov function $V$ exists for fixed point $0$, then $x(t) = 0$ is <em>Lyapunov stable</em>.</p> </li> <li> <p>If furthermore \(D_x V(x) \cdot F(x(t)) &lt; 0,\) then the equilibrium is <em>asymptotically stable</em>.</p> </li> <li> <p>Finally, if there are also $\alpha, \beta, \varepsilon &gt;0$, $p \geq 1$ such that</p> <ul> <li>$\alpha \norm{x}^p \leq V(x) \leq \beta \norm{x}^p$, and</li> <li>$D_xV(x) \cdot F(x(t)) \leq -\varepsilon V(x)$,</li> </ul> <p>then the point is <em>exponentially stable</em>.</p> </li> </ol> <hr/> <p>Thus, we see that stability can be determined by considering properties of the Lyapunov function, rather than by looking at the trajectory directly! There’s more to say about this, but it seems as though basic Lyapunov theory is pretty well-covered online already. Especially helpful sources for me were Haddad et al.’s textbook <a href="https://www.jstor.org/stable/j.ctvcm4hws">Nonlinear Dynamical Systems and Control: A Lyapunov-Based Approach</a>, Slotine and Li’s textbook <a href="https://lewisgroup.uta.edu/ee5323/notes/Slotine%20and%20Li%20applied%20nonlinear%20control-%20bad%20quality.pdf">Applied Nonlinear Control</a>, and this <a href="https://byjus.com/maths/lyapunov-functions/">blog post</a>.</p> <p>However, there is one final point which is worth thinking about before we move on. In particular, asymptotic stability only requires that there exists <em>some</em> ball $B_\delta(0)$ in which every trajectory converges to 0. It isn’t obvious exactly which balls work for this, and this matters a lot in later sections, when we want to make sure that our system will converge to the equilibrium from a broad range of starting positions.</p> <p>To start to get at this, note that condition (3) of the Lyapunov Function definition implies that the derivative $\frac{d V(x(t))}{dt}$ is non-positive. In other words, $V(x(t))$ is a nonincreasing function. Then we can use the fact that $V(x(t)) \leq V(x(0))$ for all $t$ to make the following statement:</p> <hr/> <h4 id="lemma--invariance-of-sublevel-sets-of-lyapunov-functions"><strong>Lemma : Invariance of Sublevel Sets of Lyapunov Functions</strong></h4> <p>Consider any Lyapunov function $V$, and a corresponding <em>sublevel set</em> $S_\beta = \{ x \in \mathbb{R}^n | V(x) \leq \beta \}$ satisfying $S_\beta \subseteq \mathcal{D}$. Then $S_\beta$ is <em>forward-invariant</em>. That is, if $x(0) \in S_\beta$, then $x(t) \in S_\beta $ for all $t$.</p> <p>Moreover, if property $(2)$ of Lyapunov’s Stability Theorem holds, then $x(t) \to 0$ for all $x(0) \in S_\beta$ if either:</p> <ol> <li>$\mathcal{D}$ is bounded, or</li> <li>$V(x)$ is <em>radially unbounded</em>—that is, as $\norm{x}\to\infty$, $V(x) \to \infty$.</li> </ol> <hr/> <p><strong><em>Proof</em></strong>.</p> <p>Say that this statement were false. Then, since the trajectory $x(t)$ is continuous, there must exist some time $s$ such that $x(s)$ is not in $S_\beta$. Then, since by construction $V(x(0)) &lt; \beta$ and $V(x(s)) &gt; \beta$, we find</p> \[\frac{ V(x(s)) - V(x(0)) } { s - 0 } := m &gt; \frac{\beta - \beta}{s} = 0.\] <p>Thus, there is some positive secant slope $m$ connecting these two points. But since $V$ and $x$ are both continuously differentiable by construction, $V(x(t))$ is as well, and so we can apply the mean value theorem to find that there must be some time $s_0$ such that</p> \[\frac{d V(x(t))}{dt} = D_xV(x) \cdot F(x(t)) = m &gt; 0,\] <p>which contradicts our definition of a Lyapunov function. Thus, we find that each $S_\beta$ must be forward-invariant.</p> <p>Consider then the second condition (I’m still working on this part: Come back later!).</p> <h1 id="part-ii-control-lyapunov-functions">Part II: Control Lyapunov Functions</h1> <p>For this part, we are going to tweak our setting slightly. Say now that we can apply a <em>control input</em> $u(t)$ which lets us effect the state $x(t)$—for example, say that we had a motor which could supply some force in a direction of our choosing at each timestep. Unless otherwise mentioned, we let $u : \mathcal{D} \to \mathcal{U} \subseteq \mathbb{R}^m$ be a <em>feedback control</em> for our system: that is, $u$ is a function of $x$, $u(x)$ is chosen to apply a correction at point $x$ which pushes it towards equilibrium, and $\mathcal{U}$ is the space of all possible inputs.</p> <p>With this setup, we have a dynamical system which is a function of both the state and control input:</p> \[\overset{.}{x}(t) = F(x(t), u(t)), \;\; \forall t \in [0, \tau).\] <p>Our goal is now to figure out whether our control input will drive the system to equlibrium—or, more ambitiously, to somehow derive an optimal controller for this dynamical system.</p> <p>One naive approach is to just choose some feedback controller $u(x)$ and let $\overset{\sim}{x} := [x,u(x)]$ be a new state vector. Then</p> \[\overset{.}{\overset{\sim}{x}}(t) = \overset{\sim}{F}(\overset{\sim}{x}(t)) := F(x(t), u(x))\] <p>is a dynamical system which can be analyzed using the usual Lyapunov functions from <a href="#part-i--lyapunov-functions">Part I</a>. However, this methodology has some major problems. Most pressingly, if it’s <em>hard</em> to randomly pick a $u(x)$ that stabilizes the system, then we have no way to figure out ‘good’ choices of $u$ other than just by plugging in a ton of options. This motivates Control Lyapunov Functions—we want to create an object which can allow us to <em>locate</em> good controls for a given problem.</p> <p>We formally define a Control Lyapunov Function as follows:</p> <hr/> <h4 id="definition-control-lyapunov-function-clf"><strong>Definition (Control Lyapunov Function (CLF)):</strong></h4> <p>Let $F(x(t), u(t))$ be a controlled dynamical system defined as above. Then a continuously differentiable function $V: \mathcal{D} \to \mathbb{R}$ is called a control Lyapunov function for the system if the following hold:</p> <ol> <li>$V(0) = 0$.</li> <li>$V(x) &gt; 0 \;\; \forall x \in \mathcal{D} \setminus {0}$.</li> <li>$ \underset{u \in \mathcal{U}}{\inf} \; D_xV(x) \cdot F(x, u) &lt; 0$ for all $x \in \mathcal{D} \setminus {0}$.</li> </ol> <hr/> <p>The first two conditions are identical to a normal Lyapunov function. The third is a little stranger. It requres that, for every point $x$, there exists some control input $u$ that causes the directional derivative to be negative. We can think of this as implying the existence of a stabilizing feedback controller. In particular, taking</p> \[u^*(x) \in \{u \in \mathcal{U} | D_xV(x) \cdot F(x,u) &lt; 0\} \;\; \forall x \in \mathcal{D},\] <p>where each of these sets is guaranteed to be nonempty by condition (3), we find that the dynamical system</p> \[\overset{.}{x}(t) = F(x(t), u^*(x(t)))\] <p>is asymptotically stable, since $V$ is a Lyapunov function satisfying criterion (2) from Lyapunov’s Stability Theorem. Conversely, if condition (3) is not met, then there must be some $x$ such that NO control can stabilize it, and so there does not exist a stable controller. Thus, we can interpret the existence of a CLF as a necessary and sufficient condition for the dynamical system to be asymptotically stabilizable using a feedback controller. We summarize this in the following theorem:</p> <hr/> <h4 id="theorem-stability-of-controlled-systems"><strong>Theorem (Stability of Controlled Systems)</strong></h4> <p>Let $\overset{.}{x}(t) = F(x(t),u)$ be a controlled dynamical system defined as above. Then this system is asymptotically stabilizable by a feedback controller if and only if there exists a Control Lyapunov Function for the system.</p> <hr/> <p>Now, in some sense, we’ve solved our problem of characterizing optimal controllers. If we can develop an algorithm which can sample members of $\{u \in \mathcal{U} | D_xV(x) \cdot F(x,u) &lt; 0\}$, then we can use this scheme to create a controller. However, it is not always clear how to do this in practice. In the following section, we identify a strategy for calculating optimal controllers in the special case of a linear controller.</p> <h3 id="special-case--linear-control">Special Case : Linear Control</h3> <p>Assume that we are working with an <em>affine control system</em> governed my the dynamics</p> \[\overset{.}{x}(t) = F(x(t)) + G(x(t)) u(t) \;\; \forall t \in [0,\tau)\] <p>for some $F : \mathbb{R}^n \to \mathbb{R}^n$ and $G: \mathbb{R}^n \to \mathbb{R}^{n \times m}$, both smooth. Assume also for the time being that the control space \(\mathcal{U} = \mathbb{R}^m\). For a simple example of such a system, say that $G(x) = I$ for all $x$. Then at each time our control input corresponds to moving the agent in any direction.</p> <p>In this setting, we can make a stronger statement about the existence of optimal control:</p> <hr/> <h4 id="theorem-affine-control-lyapunov-function"><strong>Theorem (Affine Control Lyapunov Function)</strong></h4> <p>Under an affine control system $\overset{.}{x}(t) = F(x(t)) + G(x(t)) u(t)$ defined as above, a positive-definite, radially unbounded function $V:\mathbb{R}^n \to \mathbb{R}$ is a Control Lyapunov Function if and only if</p> \[D_xV(x) \cdot F(x(t)) &lt; 0 \;\; \forall x \in \mathcal{R},\] <p>where</p> \[\mathcal{R} := \{x \in \mathbb{R}^n | x \neq 0, D_xV(x) \cdot G(x(t)) = \mathbf{0}\}.\] <hr/> <p><em><strong>Proof.</strong></em></p> <p>Consider condition (3) of a CLF, that is:</p> <p>$ \underset{u \in \mathcal{U}}{\inf} \; D_xV(x) \cdot F(x, u) &lt; 0$ for all $x \in \mathcal{D} \setminus \{0\}.$</p> <p>By plugging in the affine control to this expression, we see that it’s equivalent to</p> \[\underset{u \in \mathcal{U}}{\inf} \; D_xV(x) \cdot [F(x(t)) + G(x(t)) u(t)]\] \[= \underset{u \in \mathcal{U}}{\inf} \; \left[ D_xV(x) \cdot F(x(t)) + D_xV(x) \cdot G(x(t)) u(t) \right].\] <p>If $D_xV(x) \cdot G(x(t)) \neq \mathbf{0}$, then there must be some non-zero entry at index $i$ with sign $\varepsilon$. Then taking $u_i \to -\varepsilon \cdot \infty$, we see that the entire expression can approach $-\infty &lt; 0$.</p> <p>Thus, the only conditions that we care about occur when $D_xV(x) \cdot G(x(t)) = \mathbf{0}$—that is, for points in $\mathcal{R}$. In these cases, we require that</p> \[\underset{u \in \mathcal{U}}{\inf} \; D_xV(x) \cdot F(x(t)) &lt; 0,\] <p>which is exactly our result.</p> <p>With this lemma, we can explicitly design a controller which globally (for any $x \in \mathbb{R}$ stabilizes our system). We define the following controller:</p> <hr/> <h4 id="theorem--linear-feedback-controller"><strong>Theorem : Linear Feedback Controller</strong></h4> <p>For the affine control system $\overset{.}{x}(t)=F(x(t))+G(x(t)) u(t)$ defined above, there exists a feedback controller $\phi : \mathbb{R}^n \to \mathbb{R}^m$ given by the following:</p> \[\phi(x) = \begin{cases} - \left( c_0 + \frac{\alpha(x) + \sqrt{\alpha^2(x) + \left[ \beta^T(x) \beta(x) \right]^2}}{\beta^T(x) \beta(x)} \right) \beta(x), &amp; \beta(x) \neq 0, \\ 0 &amp; \beta(x) = 0, \end{cases}\] <p>where $c_0 \geq 0$ is any scalar,</p> \[\alpha(x) := D_xV(x) \cdot F(x),\] <p>and</p> \[\beta(x) := \left[ D_xV(x) \cdot G(x) \right]^T.\] <hr/> <p><strong><em>Proof.</em></strong></p> <p>We just plug into</p> \[D_xV(x) \cdot [F(x) + G(x) \phi(x)] = \alpha(x) + \beta^T(x) \phi(x).\] <p>If $\beta(x) = 0$, then we find $D_xV(x) = D_xV(x) \cdot F(x) &lt; 0$ by the Affine Control Lyapunov Theorem.</p> <p>Otherwise, we substitute in $\phi$ to find</p> \[= \alpha(x) - \beta^T(x) \beta(x) \left( c_0 + \frac{\alpha(x) + \sqrt{\alpha^2(x) + \left[ \beta^T(x) \beta(x) \right]^2}}{\beta^T(x) \beta(x)} \right)\] \[= -\beta^T(x) \beta(x) c_0 - \sqrt{\alpha^2(x) + \left[ \beta^T(x) \beta(x) \right]^2} &lt; 0,\] <p>since both of these terms are negative by construction. Thus, we find that the derivative is negative, and so this is indeed a stabilizing controller.</p> <p>But what’s happening here intuitively? Consider for simplicity when $c_0 = 0$.</p> <p>Then let’s consider two examples. First, say that $G(x) = I$; that is, we just move in direction $u$. Then $\beta(x) = D_xV(x)$, so our control input has us move in the direction of steepest descent in $V$. This looks like a gradient descent! We have some ‘strength’ given by the big constant term, which we then apply to the direction which moves us towards equilibrium the fastest. Notice that our construction ensures that our derivative $\frac{d V(x(t))}{dt}$ is $-\sqrt{\alpha^2(x) + \norm{\beta}^4}$. So we see that this is the rate at which we will move down $V$.</p> <p>As another example, say that we have a more constrained $G$ given by $G(x) = [1,0]^T$. Then our control input is a one-dimensional displacement in the $x$ axis. Then $\beta(x) = \frac{d V(x)}{d x_1}$. Thus, our optimal control has us moving in the direction that decreases $V$ as much as possible, given that we can only control along the $x$ axis. Namely, if the partial along the $x$ axis is positive, we move in the negative direction, and vice-versa. Thus, we see that this looks like a constrained version of the previous example,</p> <p>Thus, we find that we can create optimal feedback controllers for affine systems.</p> <h3 id="note--local-stabilizability">Note : Local Stabilizability</h3> <p>One question that seems relevant is, what if we can’t assume that $\mathcal{U} = \mathbb{R}^m$? I’ll fill this in later (Section TBD), but I think that the main idea is that obviously global stability could fail, because we might get too far away from the equilibrium to reasonably control there. However, if we are operating in a bounded set $\mathcal{D}$, then under certain regularity assumptions (like that everything we care about is Lipschitz on $\mathcal{D}$), we can bound the size of the optimal controller given above. Thus, we can create a controller constrained to $\mathcal{U}$ which is optimal, and perhaps we can even say the ‘weakest’ controller which can still work!</p> <h1 id="part-iii--control-barrier-functions">Part III : Control Barrier Functions</h1> <p>Thus far, we have mostly been concerned with studying the <em>stability</em> of equilibria and controlled equilibria. However, there are often other concerns worth accounting for during planning.</p> <p>In this section, we consider <em>safety</em> concerns in dynamical systems. In particular, it is possible that some $x(t)$ are ‘bad’ states—for example, some orientations of a robotic arm could cause it to hit an object, or an autonomous vehicle being in certain regions of space could be dangerous if those regions are also occupied by humans. In general, we could imagine assigning a ‘badness’ score to every point in our state space, with negative badness scores being avoided. Then we could cast the problem of ensuring safety as enforcing that we remain in ‘good’ zones; that is, those with positive score.</p> <p>This idea enables us to define a <em>Barrier Function</em> (technically, a Zeroing Barrier Function, see <a href="https://ieeexplore.ieee.org/abstract/document/7782377?casa_token=Su26OE1ZV1EAAAAA:KMlvhbhQq0wb76ixit1QMQgIVqZEQj8Vf4fTlfewe5_auWi48Zb-nSSJ7mxopBuDgF0bLT3BHw">Ames et al.</a>):</p> <hr/> <h4 id="definition--barrier-function"><strong>Definition : Barrier Function</strong></h4> <p>A continuously differentiable function $h : \mathcal{D} \to \mathbb{R}$ is called a <em>Barrier Function</em> for $\mathcal{C} = \{ x\in \mathbb{R}^n | h(x) \geq 0 \}$ if $\mathcal{C} \subseteq \mathcal{D}$ and there exists some strictly increasing function $\alpha : \mathbb{R} \to \mathbb{R}$ with $\alpha(0) = 0$ such that</p> \[\frac{d h(x(t))}{d t} = D_xh(x) \cdot F(x) \geq -\alpha(h(x)).\] <hr/> <p>This definition implies that, at the boundary $\partial \mathcal{C} = \{x \in \mathbb{R}^n | h(x) = 0 \}$,</p> \[\frac{d h(x(t))}{d t} \geq -\alpha(0) = 0,\] <p>so $h(x)$ will not leave $\mathcal{C}$ if it is already inside it. Thus, we see that this definition enforces forward invariance as we desire. In fact, we can actually get an even stronger condition. Decause $\alpha$ is strictly increasing, if we are outside of our set $\mathcal{C}$, then we find that</p> \[\frac{d h(x(t))}{d t} \geq -\alpha(0) &gt; 0,\] <p>so we will be travelling towards $\mathcal{C}$ whenever we are outside of it. This can be formalized as a Lyapunov function as follows:</p> <hr/> <h4 id="theorem--bf-to-lyapunov-function"><strong>Theorem : BF to Lyapunov Function</strong></h4> <p>Say that $h(x)$ is a Barrier Function for $\mathcal{C}$ with domain $\mathcal{D}$. Then define $V_\mathcal{C}$ as follows:</p> \[V_\mathcal{C}(x) = \begin{cases} 0, &amp; \text{ if } x \in \mathcal{C},\\ -h(x) &amp; \text{ if } x \in \mathcal{D} \setminus \mathcal{C}. \end{cases}\] <p>$V_\mathcal{C}$ is a Lyapunov function for our problem. Moreover, $\mathcal{C}$ is an asymptotically stable fixed set.</p> <hr/> <p>Here we generalize a Lyapunov function to a set, rather than a particular point—this causes no real issues, but it is a worthwhile exercise to make sure that everything still makes sense when we do this replacement.</p> <p>It actually turns out that the converse of this theorem is also true if $\mathcal{C}$ is nonempty and compact. That is, we also have that $\mathcal{C}$ is a forward-invariant set if and only if you can create a barrier function for it!</p> <p>However, Barrier Functions don’t allow us to steer our dynamical system to safe regions. To do this, assume now that we are in an affine control system</p> \[\overset{.}{x}(t) = F(x(t)) + G(x(t)) u(t) \;\; \forall t \in [0,\tau)\] <p>analogously to defining CLFs from LFs, we define the notion of a <em>Control Barrier Function</em>:</p> <hr/> <h4 id="definition--control-barrier-function"><strong>Definition : Control Barrier Function</strong></h4> <p>A continuously differentiable function $h : \mathcal{D} \to \mathbb{R}$ is called a <em>Control Barrier Function</em> for $\mathcal{C} = \{ x\in \mathbb{R}^n | h(x) \geq 0 \}$ if $\mathcal{C} \subseteq \mathcal{D}$ and there exists some strictly increasing function $\alpha : \mathbb{R} \to \mathbb{R}$ with $\alpha(0) = 0$ such that</p> \[\underset{u \in \mathcal{U}}{\text{sup}} \frac{d h(x(t))}{d t} = \underset{u \in \mathcal{U}}{\text{sup}} D_xh(x) \cdot F(x) + D_x h(x) \cdot G(x) u \geq -\alpha(h(x)).\] <hr/> <p>This looks really similar to our definition of the CLF. Now, we see that there must exist some control input that will render each induce a barrier function when the corresponding control is substituted in. The analysis here is exactly the same as in the CLF case (indeed, we can define a CLF as in the previous section as the negation of our CBF), so we omit it here. The important thing is that we can design a control which allows us to ensure that we remain in a safe set.</p> <h1 id="part-iv--controlling-with-cbfs-and-clfs">Part IV : Controlling With CBFs and CLFs</h1> <p>Let’s tie this all together by defining concrete</p> <p>First, say that we are given a target feedback controller $\phi(x)$. We want to derive a controller $u$ which is as close to $\phi$ as possible, but with the added constraint that it must be safe with respect to some set $\mathcal{C}$. We can get this by solving the <a href="https://en.wikipedia.org/wiki/Quadratic_programming">Quadratic Program</a> (QP):</p> \[\begin{align*} u(x) &amp;= \underset{u \in \mathcal{U}}{\text{arg min}} \frac{1}{2} \norm{u - \phi(x)}^2 \\ &amp; \text{ s.t. } D_xh(x) \cdot F(x) + D_x h(x) \cdot u \geq -\alpha (h(x)). \end{align*}.\] <p>According to work outside the scope of this blog post (see <a href="https://coogan.ece.gatech.edu/papers/pdf/amesecc19.pdf">Ames et al., 2019.</a>), this problem has a closed-form solution when $\mathcal{U} = \mathbb{R}^n$.</p> <p>More generally, we can use this strategy to assign a controller that both stabilizes the system (as determined by some CLF $V$) AND offers safety guaranteed by $h$. In particular, the following QP works:</p> \[\begin{align*} u(x) &amp;= \underset{(u, \delta) \in \mathbb{R}^{m+1}}{\text{arg min}} \frac{1}{2} u^T H(x) u + p \delta^2 \\ &amp; \text{ s.t. } D_x V(x) \cdot F(x) + D_x V(x) \cdot u \leq -\gamma (V(x)) + \delta\\ &amp; D_xh(x) \cdot F(x) + D_x h(x) \cdot u \geq -\alpha (h(x)), \end{align*}\] <p>where $\gamma$ is another class $\mathcal{K}$ function, $H(x)$ is any positive definite matrix (so that term drives $\norm{u} \to 0$) and $\delta$ is a relaxation parameter which ensures that the program is solvable. Having the term $p \delta^2$ in the constraint drives this relaxation to $0$. The solution to this problem is guaranteed to be safe, and is likely to be stable (more so as we take $\delta \to 0$).</p> <h1 id="conclusion">Conclusion</h1> <p>Thus, we find that the problem of controlling subject to some safety constraint, or optimally controlling with both a stability and safety objective, can be achieved using CLFs and CBFs, and solved using existing tools for Quadratic Programming. For example, see the <a href="https://www.ll.mit.edu/partner-us/available-technologies/control-barrier-function-toolbox">Control Barrier Function Toolbox</a> from Lincoln Labs.</p> <p>It is perhaps worthwhile to see some concrete examples of CLFs and CBFs in the wild. I plan to cover this in a future blog post, focusing in particular on a paper out of my lab (<a href="https://ieeexplore.ieee.org/abstract/document/10354416?casa_token=6jyiwNV7sCEAAAAA:5BLuAUeRw1ZmuZaxHnD_YfWKkL0wZqbVP8pHQNU8xamrJAbb9cJMJGvkxzNIYrNsIZ59Or0">Cavorsi et al., 2023. </a>).</p> <p>In any case, thanks for reading.</p> <p>—Thomas</p> <h1 id="references--further-reading">References + Further Reading</h1> <p>Here are a few of the books and papers that I found especially helpful. Especially clear writing is marked in <strong>bold</strong>.</p> <h4 id="textbooks">Textbooks:</h4> <ul> <li> <p><a href="https://lewisgroup.uta.edu/ee5323/notes/Slotine%20and%20Li%20applied%20nonlinear%20control-%20bad%20quality.pdf">Slotine and Li, 1991. Applied Nonlinear Control</a></p> </li> <li> <p><strong><a href="https://www.jstor.org/stable/j.ctvcm4hws">Haddad and Chellaboina, 2008. Nonlinear Dynamical Systems and Control: A Lyapunov-Based Approach</a></strong></p> </li> </ul> <h4 id="papers-chronological">Papers (Chronological):</h4> <ul> <li> <p><a href="https://www.sciencedirect.com/science/article/pii/0362546X83900494">Artstein, 1982. Stabilization with Relaxed Controls</a></p> </li> <li> <p><a href="https://epubs.siam.org/doi/abs/10.1137/0321028?casa_token=UBCu6QQcDNQAAAAA:gnQ5sEIIyS-uXT8bzuob6UIffSQjAbhfVmpw8qx_cxOJ-RmMJT25gE1Hb5iwxCJqXlYsIl9J">Sontag, 1983. A Lyapunov-Like Characterization of Asymptotic Controllability</a></p> </li> <li> <p><a href="http://www.sontaglab.org/FTPDIR/art-sycon8903.pdf">Sontag, 1989. A ‘Universal’ Construction of Artstein’s Theorem on Nonlinear Stabilization</a></p> </li> <li> <p><strong><a href="https://ieeexplore.ieee.org/abstract/document/7782377?casa_token=Su26OE1ZV1EAAAAA:KMlvhbhQq0wb76ixit1QMQgIVqZEQj8Vf4fTlfewe5_auWi48Zb-nSSJ7mxopBuDgF0bLT3BHw">Ames et al., 2016. Control Barrier Function Based Quadratic Programs for Safety Critical Systems</a></strong></p> </li> <li> <p><a href="https://coogan.ece.gatech.edu/papers/pdf/amesecc19.pdf">Ames et al., 2019. Control Barrier Functions: Theory and Applications</a></p> </li> <li> <p><a href="https://ieeexplore.ieee.org/abstract/document/9197109?casa_token=HEt2jergrKEAAAAA:oStQJxHycKbc4iCHloCpSi62P8oWoPqILA4k_tcUEYNrMS76EK40c4lyLcJSjdBnA5LlL7E">Capelli et al., 2020. Connectivity Maintenance: Global and Optimized Approach Through Control Barrier Functions</a></p> </li> <li> <p><a href="https://ieeexplore.ieee.org/abstract/document/10354416?casa_token=6jyiwNV7sCEAAAAA:5BLuAUeRw1ZmuZaxHnD_YfWKkL0wZqbVP8pHQNU8xamrJAbb9cJMJGvkxzNIYrNsIZ59Or0">Cavorsi et al., 2023. Multirobot Adversarial Resilience Using Control Barrier Functions</a></p> </li> </ul> ]]></content><author><name></name></author><category term="control"/><category term="lyapunov"/><category term="control"/><category term="dynamical-systems"/><summary type="html"><![CDATA[A brief introduction to Lyapunov theory as I currently understand it.]]></summary></entry><entry><title type="html">A Brief List of MOOCs</title><link href="https://tkaminsky.github.io/blog/2024/mooc-guide/" rel="alternate" type="text/html" title="A Brief List of MOOCs"/><published>2024-06-11T16:40:16+00:00</published><updated>2024-06-11T16:40:16+00:00</updated><id>https://tkaminsky.github.io/blog/2024/mooc-guide</id><content type="html" xml:base="https://tkaminsky.github.io/blog/2024/mooc-guide/"><![CDATA[<p>A lot of people right now seem to be interested in self-studying course material, and I think that Massive Open Online Courses (MOOCs) are a great way to organize your learning. We’re living in a sort of golden age for free online education, but it can be pretty difficult to navigate all of your options. Here I lay out a sort of guide for the types of online courses that I’ve encountered, what I understand as the strengths and limitations of each type, and some good first courses to get started with online learning.</p> <h2 id="the-official-ones"><strong>The Official Ones</strong></h2> <p>Perhaps the gold standard for MOOCs are those that re-upload college-level courses in their (near) entirety. These have the benefit of being credentialed by some big-name institutions and being really well-organized, but a lot of them don’t take advantage of their online formats, and it can still be a struggle to engage with them because there is so much less explicit feedback than in an in-person class.</p> <p>A lot of them also have annoying, optional `certificate’ programs, which seem to me like a bit of a waste of money. I’m not sure that a lot of employers value a certified online course as much as, say, a course taken at a university or technical school, so I’d usually stick to auditing it for free instead. I describe a few of the major sites below:</p> <h4 id="mit-opencourseware"><a href="https://ocw.mit.edu/">MIT OpenCourseWare</a></h4> <p>OpenCourseWare (OCW) is probably my favorite website in general for taking online courses. They have a large list of offerings, most of which are really well-documented, including problem sets, exams, and sections.</p> <p>However, it definitely has a few issues. Though there are over 1,200 courses listed at the time of writing, only 250 or so have lecture videos, with the rest mostly being lecture notes. To save yourself some pain, just filter for courses with recordings to begin with. I’ve also found that, though some courses are really beautifully recorded, other videos are a little bad—I sometimes find it hard to read the board, especially in older courses.</p> <p>The videos also aren’t uniformly distributed across subjects. Unsurprisingly, most of the offerings are in Computer Science, followed by math and physics. So if you’re interested in other one of these neglected fields, especially in the social sciences or humantities, this might not be the platform for you.</p> <p>Here are some example courses that I think do a great job:</p> <ul> <li><a href="https://ocw.mit.edu/courses/6-006-introduction-to-algorithms-fall-2011/pages/syllabus/">6.006: Introduction to Algorithms</a></li> <li><a href="https://ocw.mit.edu/courses/6-849-geometric-folding-algorithms-linkages-origami-polyhedra-fall-2012/pages/syllabus/">6.849: Geometric Folding Algorithms: Linkages, Origami, Polyhedra</a></li> <li><a href="https://ocw.mit.edu/courses/cms-611j-creating-video-games-fall-2014/pages/syllabus/">CMS.611J: Creating Video Games</a></li> </ul> <h4 id="mit-open-learning-library"><a href="https://openlearning.mit.edu/">MIT Open Learning Library</a></h4> <p>OLL is a sort of super-OCW, in that a lot of these courses are, or used to be, listed on OCW. They tend to be more polished and better-utilize online tools, but this also can result in a less rigorous experience, especially where homework is concerned. Still, the autograders and professionally-recorded lectures make the platform a really great way to have an organized experience.</p> <p>It also has a strange relationship with EdX—like half of the courses are listed there as well, and so it isn’t always obvious how to</p> <p>Here are some courses that you might be interested in:</p> <ul> <li><a href="https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/course/">6.036: Introduction to Machine Learning</a></li> <li><a href="https://openlearninglibrary.mit.edu/courses/course-v1:OCW+18.06SC+2T2019/about">18.06SC: Linear Algebra</a></li> <li><a href="https://mitxonline.mit.edu/courses/course-v1:MITxT+8.02.1x/?utm_source=openlearning&amp;utm_medium=referral&amp;utm_campaign=mitx_catalog">8.02.1x: Electricity and Magnetism: Electrostatics</a></li> </ul> <h4 id="coursera"><a href="https://www.coursera.org/">Coursera</a></h4> <p>One of the first massive repositories for online courses, Coursera has perhaps the largest catalog of college-level courses for free online. Like OLL, it has a certificate program that you can buy if you really want to, but you can learn basically everything for free. It also has a wider variety of courses than OCW, and they are taught by a huge number of schools, rather than just MIT.</p> <p>The website was created by professors at Stanford, so it has especially high-quality courses from west-coast schools. However, they also allow courses to be uploaded by any school, or even by companies, I’ve noticed that they vary a ton in quality. Some of the courses uploaded by companies, in particular, seem more like lessons in pro-tech jargon than useful college-level courses, so use at your own risk. They’ve also leaned really hard into certifications and ‘tracks’ which are expensive and make bold claims about ‘skilling-up’, which I find slightly overhyped.</p> <p>Still, there are a lot of interesting courses on offer. Here are a few:</p> <ul> <li><a href="https://www.coursera.org/learn/crypto">Cryptography I</a> (Stanford)</li> <li><a href="https://www.coursera.org/learn/introduction-to-calculus">Introduction to Calculus</a> (University of Sydney)</li> <li><a href="https://www.coursera.org/learn/mythology">Greek and Roman Mythology</a> (UPenn)</li> <li><a href="https://www.coursera.org/learn/game-theory-1">Game Theory</a> (Stanford)</li> </ul> <h4 id="edx"><a href="https://www.edx.org/?utm_source=google&amp;utm_campaign=18736834479&amp;utm_medium=cpc&amp;utm_term=edx&amp;hsa_acc=7245054034&amp;hsa_cam=18736834479&amp;hsa_grp=140243978342&amp;hsa_ad=631521652739&amp;hsa_src=g&amp;hsa_tgt=kwd-89882436&amp;hsa_kw=edx&amp;hsa_mt=e&amp;hsa_net=adwords&amp;hsa_ver=3&amp;gad_source=1&amp;gclid=CjwKCAjw65-zBhBkEiwAjrqRMEOBsQ6zedEbtu9jPhubnOaQc7q21glYqTX5wQE2d33lsrHlRsTv5BoCFt4QAvD_BwE">edX</a></h4> <p>edX was Harvard and MIT’s answer to Coursera in 2012. It is very similar to Coursera and OLL in structure</p> <p>In terms of cons, it suffers at times from a lack of rigor, though some courses, like the incredible CS50X, manage to be surprisingly similar to their in-person variants. Also, because they allow courses to be uploaded by any school, or even by companies, I’ve noticed that courses vary a ton in quality. Some of the courses uploaded by companies, in particular, seem more like lessons in pro-tech jargon than useful college-level courses.</p> <p>Here are some good courses to try out:</p> <ul> <li><a href="https://www.edx.org/learn/computer-science/harvard-university-cs50-s-introduction-to-computer-science?index=product&amp;queryID=dae63c7c0a87115e4045c23228b90ab4&amp;position=4&amp;results_level=second-level-results&amp;term=&amp;objectID=course-da1b2400-322b-459b-97b0-0c557f05d017&amp;campaign=CS50%27s+Introduction+to+Computer+Science&amp;source=edX&amp;product_category=course&amp;placement_url=https%3A%2F%2Fwww.edx.org%2Fsearch">CS50: Introduction to Computer Science</a> (Harvard)</li> <li><a href="https://www.edx.org/learn/architecture-history/massachusetts-institute-of-technology-a-global-history-of-architecture?index=product&amp;objectID=course-89508395-9972-49c6-9ff4-1e276f9cc83b&amp;webview=false&amp;campaign=A+Global+History+of+Architecture&amp;source=edX&amp;product_category=course&amp;placement_url=https%3A%2F%2Fwww.edx.org%2Flearn%2Farchitecture-history">A Global History of Architecture</a> (MIT)</li> <li><a href="https://www.edx.org/learn/neuroscience/harvard-university-fundamentals-of-neuroscience-part-1-the-electrical-properties-of-the-neuron?index=product&amp;queryID=558d4a3a0a5acd03c64e59f617cb681b&amp;position=1&amp;results_level=second-level-results&amp;term=neuroscience&amp;objectID=course-f8042b37-6a21-4afa-b9e2-6f51dcd694db&amp;campaign=Fundamentals+of+Neuroscience%2C+Part+1%3A+The+Electrical+Properties+of+the+Neuron&amp;source=edX&amp;product_category=course&amp;placement_url=https%3A%2F%2Fwww.edx.org%2Fsearch">Fundamentals of Neuroscience</a> (Harvard)</li> <li><a href="https://www.edx.org/learn/happiness/university-of-california-berkeley-the-science-of-happiness?index=product&amp;queryID=cda429aabf2c8455f4c7092ec152d84f&amp;position=2&amp;results_level=second-level-results&amp;term=&amp;objectID=course-73484215-4007-48cd-ba90-c945cde6030d&amp;campaign=The+Science+of+Happiness&amp;source=edX&amp;product_category=course&amp;placement_url=https%3A%2F%2Fwww.edx.org%2Fsearch">The Science of Happiness</a> (Berkeley)</li> </ul> <h2 id="the-independent-websites"><strong>The Independent Websites</strong></h2> <p>There also exist a number of courses offered by parties unaffiliated with Universities. Lots of the big names in this category—<a href="https://www.udemy.com/?utm_source=adwords-brand&amp;utm_medium=udemyads&amp;utm_campaign=Brand-Udemy_la.EN_cc.US_dev&amp;campaigntype=Search&amp;portfolio=BrandDirect&amp;language=EN&amp;product=Course&amp;test=&amp;audience=Keyword&amp;topic=&amp;priority=NotSpecified&amp;utm_content=deal4584&amp;utm_term=_._ag_137319648178_._ad_634190764968_._kw_udemy_._de_c_._dm__._pl__._ti_kwd-296956216253_._li_9033311_._pd__._&amp;matchtype=b&amp;gad_source=1&amp;gclid=CjwKCAjw65-zBhBkEiwAjrqRMI42Ly4kM0rau4SLEmNmrk1iOQwwwTGIv7rKMpZ1Zi6aGeRSocLRiBoCGc0QAvD_BwE">Udemy</a>, <a href="https://www.masterclass.com/">MasterClass</a>, etc.—aren’t free, so I haven’t tried them.</p> <p>Still, there are a few I’d recommend:</p> <h4 id="khan-academy"><a href="https://www.khanacademy.org/">Khan Academy</a></h4> <p>By far the msot important online learning resource for High Schoolers, Sal Khan created a really incredible resource as far back as 2008. It’s grown into a large and exciting platform, though it focuses more on course material for younger students, rather than advanced courses. Still, if you want to study a course that you missed in high school, this is probably the place I’d start.</p> <h4 id="codecademy"><a href="https://www.codecademy.com/?g_network=g&amp;g_productchannel=&amp;g_adid=624951457609&amp;g_locinterest=&amp;g_keyword=codecademy&amp;g_acctid=243-039-7011&amp;g_adtype=&amp;g_keywordid=kwd-41065460761&amp;g_ifcreative=&amp;g_campaign=account&amp;g_locphysical=9033311&amp;g_adgroupid=70946090375&amp;g_productid=&amp;g_source={sourceid}&amp;g_merchantid=&amp;g_placement=&amp;g_partition=&amp;g_campaignid=1955172604&amp;g_ifproduct=&amp;utm_id=t_kwd-41065460761:ag_70946090375:cp_1955172604:n_g:d_c&amp;utm_source=google&amp;utm_medium=paid-search&amp;utm_term=codecademy&amp;utm_campaign=US_Brand_Exact&amp;utm_content=624951457609&amp;g_adtype=search&amp;g_acctid=243-039-7011&amp;gad_source=1&amp;gclid=CjwKCAjw65-zBhBkEiwAjrqRMHqf6b4kf_93WgGFz7Sax60_fPlpdcuaOx0mC_Dh-TVmP5-hdUZLeRoCxigQAvD_BwE">Codecademy</a></h4> <p>Codecademy is one of the classic coding education websites, and it’s still a great way for people with no coding experience to get their feet wet. The courses are really well-organized, and you can do everything online without having to worry about setup on your computer, which can be a pain in other computer science courses.</p> <p>Still, it’s not a perfect service. I’ve found that more and more courses are getting locked behind paywalls, and there aren’t great ways to get really deep technical understanding. I’d think of the courses as quick introductions to spark your interest and learn the basic syntax of whatever language you’re interested in, but you’ll eventually need to take a more involved course, or start slogging through documentation.</p> <h2 id="the-overachievers"><strong>The Overachievers</strong></h2> <p>Very rarely, a course independently uploads all of its materials, independent of a larger platform. In my experience, these tend to be really high in quality, so if you find them, you’ll probably have a pretty good time. Still, as far as I know, there isn’t a great way to search for these sorts of courses, so it may be hard to find one you’re looking for.</p> <p>They also tend to be exclusive to computer science and related fields, for obvious reasons. Finally, if you are interested in a certificate for any reason, I doubt that these programs can give you one. Still, I think they’re a great option.</p> <p>Here are some of my favorites:</p> <ul> <li><a href="https://manipulation.csail.mit.edu/Fall2023/">6.4210: Robotic Manipulation</a> (MIT)</li> <li><a href="https://cs3110.github.io/textbook/cover.html">CS 3110: Data Structures and Functional Programming</a> (Cornell)</li> <li><a href="https://spinningup.openai.com/en/latest/">Spinning Up in Deep RL</a> (OpenAI)</li> </ul> <h2 id="conclusion"><strong>Conclusion</strong></h2> <p>I hope this was helpful! Feel free to send me any other platforms that you find success with, or any thoughts about the list.</p> <p>—Thomas</p>]]></content><author><name></name></author><category term="misc"/><category term="mooc"/><category term="cs"/><category term="learning"/><category term="education"/><summary type="html"><![CDATA[Some of my favorite hosting sites for online courses, plus some general thoughts about online learning.]]></summary></entry></feed>